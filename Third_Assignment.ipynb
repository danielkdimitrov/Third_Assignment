{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<center> <h1> Third Assignment: Numerical Simulation <h1></center>\n",
    "<center> <h2> `Random Trend Model Estimation for Panel Data` <h2></center>\n",
    "\n",
    "<center> <h3> `Carlos Gonzales (536572)`<h3></center>\n",
    "\n",
    "<center> <h3> `Daniel Dimitrov (275140)`<h3></center>\n",
    "\n",
    "<center> <h3> `Jakob Schwerter (110583)` <h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things that have to be done:\n",
    "    - Fixing the problem of the random.seed: Does not work in the python code\n",
    "    - Nice plots for all the results to picture the results. \n",
    "    - Make plots flexible to parameter changes (for us n, t and beta)\n",
    "    - If possible, make the code faster / easier\n",
    "    - Count the time python code needs\n",
    "    - Comment on which program to prefer\n",
    "    - Check typos\n",
    "    - Loading a dataset to check the simulation results for a empirical dataset\n",
    "    - write an executive summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipystata # https://github.com/TiesdeKok/ipystata - We kind of helped so that ipystata works on mac as well :)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#from scipy import stats\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "# Those packages where not used in the end, but we keep it to find them easier for later projects\n",
    "\n",
    "\n",
    "\n",
    "# pip install ipystata --upgrade --force-reinstall\n",
    "# Check if a new version is online before running the code.\n",
    "# Might be useful and does not take much time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TerminalIPythonApp] WARNING | Subcommand `ipython install-nbextension` is deprecated and will be removed in future versions.\n",
      "[TerminalIPythonApp] WARNING | You likely want to use `jupyter install-nbextension`... continue in 5 sec. Press Ctrl-C to quit now.\n",
      "downloading https://bitbucket.org/ipre/calico/downloads/calico-spell-check-1.0.zip to /var/folders/fq/w4v55vm52pvg18kzm9tvtddm0000gn/T/tmpSXdutq/calico-spell-check-1.0.zip\n",
      "extracting /var/folders/fq/w4v55vm52pvg18kzm9tvtddm0000gn/T/tmpSXdutq/calico-spell-check-1.0.zip to /usr/local/share/jupyter/nbextensions\n"
     ]
    }
   ],
   "source": [
    "#install spellchecker\n",
    "!ipython install-nbextension https://bitbucket.org/ipre/calico/downloads/calico-spell-check-1.0.zip\n",
    "#after you install it, activate it executing the next step.     \n",
    "#this video provides a demo of how it works after you install it: https://www.youtube.com/watch?v=Km3AtRynWFQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(['base/js/utils'],\n",
       "function(utils) {\n",
       "   utils.load_extensions('calico-spell-check');\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "require(['base/js/utils'],\n",
    "function(utils) {\n",
    "   utils.load_extensions('calico-spell-check');\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Motivation\n",
    "\n",
    "We construct a simulation for a random-trend (RT) model as defined by [Wooldrige 2010](https://mitpress.mit.edu/books/econometric-analysis-cross-section-and-panel-data),  section 11.7.1. The model is an extension of the [first-difference (FD) model](https://en.wikipedia.org/wiki/First-difference_estimator) for panel-data and overcomes some of its deficiencies. Our main interest is to find an unbiased estimator after a linear random-trend has already introduced some bias in the classical panel estimation methods.\n",
    "\n",
    "## Basic Panel Data Model\n",
    "\n",
    "The basic model to estimate a dependent variable follows a linear panel data specification. The specific model is as follows:\n",
    "\n",
    "$$y_{it} = \\beta \\cdot x_{it} + u_{it} \\quad (1) $$\n",
    "\n",
    "As we are working with a panel, the index $i$ stands for an individual (person, company, country, etc.), while the index $t$ represents time. The dependent and the independent variables are respectively indicated by $y_{it}$ and $x_{it}$ and vary both over the individuals and over the time dimension. The error term is represented by $u_{it}$. This can be decomposed into three terms:\n",
    "\n",
    "$$u_{it} = \\alpha_i + e_{it} $$\n",
    "\n",
    "In this context $\\alpha_i$ is an unobserved individual specific effect that stays constant for each individual and $e_{it}$ is an idiosyncratic error that changes both over time and over individuals. In practice we are mostly interested in estimating the effect of $x$ on $y$ and so we are looking for an unbiased and consistent estimation of $\\beta$. \n",
    "\n",
    "Given that both $x$ and $y$ are correlated with the first two parts of the error term, using just normal OLS without any data-transformation (pooled regression) will give biased results due to an [endogeneity bias](https://en.wikipedia.org/wiki/Endogeneity_%28econometrics%29).\n",
    "\n",
    "Two methods can be used to deal with the endogeneity problem induced by the constant error term: The [fixed-effects](https://en.wikipedia.org/wiki/Fixed_effects_model) (FE) and the first-differences (FD) method.\n",
    "\n",
    "The fixed-effects (FE) method can handle situations using the so-called within transformation, subtracting the mean from each variable corresponding to each individual. This eliminates the individual-specific effect which is constant over time (here: $\\alpha_i$) and allows for a consistent [OLS estimation](https://en.wikipedia.org/wiki/Ordinary_least_squares): \n",
    "\n",
    "$$y_{it} - \\bar{y}_{i} = \\beta \\cdot (x_{it} - \\bar{x}_{i}) + \\alpha_i - \\alpha_i + e_{it} - \\bar{e}_{i} \\quad (2)$$ \n",
    "\n",
    "with $\\bar{y}_{i} = \\sum_{t=1}^T y_{it}$, $\\bar{x}_{i} = \\sum_{t=1}^T x_{it}$ and $\\bar{e}_{i} = \\sum_{t=1}^T e_{it}$\n",
    "\n",
    "The first-differences manages to control for the bias by subtracting the past observation from the current one:\n",
    "\n",
    "$$y_{it} - y_{it-1} = \\beta \\cdot ( x_{it} - x_{it-1}) + \\alpha_i - \\alpha_{i} + (e_{it} - e_{it-1}) \\quad (3) $$\n",
    "\n",
    "One can clearly see that in both methods the $\\alpha_i$ cancels out of the regression, solving a possible correlation between $\\alpha_i$ with $y_{it} $ and $x_{it}$. Still, a crucial requirement for retrieving reliable estimates is that the independent variable and the idiosyncratic term remain uncorrelated in expectations.     \n",
    "\n",
    "Things get complicated however, when besides the unobserved constant ($\\alpha_i$), the variable of interest $x_{it}$ is correlated also with a linear trend ($g_i \\cdot t$). Keeping the variable names as before, the error term can now be decomposed as:\n",
    "\n",
    "$$u_{it} = \\alpha_i + g_i \\cdot t + e_{it} $$\n",
    "\n",
    "Now, $g_i$ is a linear trend which is specific for each individual. Note that if $y_{it}$ is a logarithm of the original variable, $g_i$ can also be interpreted as roughly the average growth rate over a period. In that case, the equation is usually referred to a random-growth model, otherwise simply as a random-trend. Overall, this presents an additional source of heterogeneity and needs to be dealt with before employing an OLS estimation.      \n",
    "\n",
    "To solve the possible bias problem due to the RT component, the literature states that in the first step we have to calculate the first-differences in order to transform the linear trend into a constant. To illustrate this more formally, taking the first-difference in the RT set-up gives\n",
    "\n",
    "$$y_{it} - y_{it-1} = \\beta \\cdot (x_{it} - x_{it-1}) + \\alpha_i - \\alpha_{i} +   g_i \\cdot t - g_{i} \\cdot (t-1) + (e_{it} - e_{it-1})$$\n",
    "\n",
    "$$ \\iff \\Delta y_{it} = \\beta \\cdot \\Delta x_{it} + g_i + \\Delta  e_{it}$$\n",
    "\n",
    "Thereafter, it is up to the researcher to continue with either the within-transformation or to first-difference again. Note that even though both are fixed-effect methods, we will be consistent with the literature to call the within-transformation the fixed-effect method. We will investigate if one of the approaches is superior to the other in means of the estimation bias of the coefficient. <span style=\"color: green\">It would be further of interest to investigate the standard errors and model selection criteria like $R^2$, AIC and BIC, but this will be left out for the future research. </span>\n",
    "\n",
    "As we saw, the linear trend has been reduced to a constant term which can now be canceled out by a second first-difference or a with-transformation. So, we have two possibilities for estimation given the RT model: \n",
    "- The **FD Method**: Taking two times the first-difference, will be named _pure_ in the tables later on\n",
    "- The **FD-FE Method**: First the first-difference, then the fixed-effect, named _mix_.\n",
    "\n",
    "First-differencing leads to the FD Method:\n",
    "\n",
    "$$\\Delta^2 y_{it} = \\beta \\cdot \\Delta^2 x_{it} + \\Delta^2 e_{it} \\quad (4) $$ where $\\Delta^2$ stands for the taking two first-differences. \n",
    "\n",
    "The alternative is to do a FE transformation for each variable by subtracting from it the mean corresponding to each individual. This leads to the FD-FE method:\n",
    "\n",
    "$$\\Delta y'_{it} = \\beta \\cdot \\Delta x'_{it} + \\Delta e'_{it} \\quad (5) $$\n",
    "\n",
    "where ' denotes that the variables are demeaned.\n",
    "\n",
    "One can clearly see that in (4) and (5) we canceled out both terms $\\alpha_i$ and $g_i \\cdot t$. Thereby $\\beta$ will not be biased even though the data initially included two different sorts of bias.\n",
    "\n",
    "On the other hand, if we had failed to take first differences in the first place, taking first the fixed-effects as in equation (3) would have given us the following:\n",
    "\n",
    "$$y_{it} - \\bar{y}_{i} = \\beta \\cdot ( x_{it} - \\bar{x}_{i}) + \\alpha_i - \\alpha_{i} +   g_i \\cdot (t - \\bar{t}) + (e_{it} - \\bar{e}_{i})$$\n",
    "\n",
    "Using a second transformation we will not be able to cancel out the time trend effect (we will confirm this in the simulation at some point). We thereby see that it is crucial to first take the first-difference and not the within-transformation.</span>\n",
    "\n",
    "Given that our group is new to python but experienced in Stata, we will first do the simulation via Stata by using the package [`ipystata`](https://github.com/TiesdeKok/ipystata). Thereby we have a known language which we can refer to as our benchmark. In a second step we will use only open software packages to replicate the results. Thereby we will comment which python code is comparable to which Stata code. Besides having the results obtained using Stata as a benchmark, we start a nice _translater_ from Stata to Python. We will also comment on the speed of both languages as well as advantages and disadvantages of the coding part.\n",
    "\n",
    "The assignment will continue as follows: (i) First we explain step by step our Data Generating Process (DGP). (ii) Then we run a simulation without a random trend to see if everything works fine. (iii) Next we run the same simulation using a constant linear trend and (iv) an individual specific trend in the data generating process. To see how robust the random-trend estimates are, we will further run two simulations using two non-linear trends.\n",
    "After that we will reproduce the results of the individual trend using open-source packages only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Simulation and Estimation with Stata\n",
    "\n",
    "## 2.1. Explanation of the Data Generating Process (DGP)\n",
    "We will first explain the main part of the simulation process. Minor changes or additions however will be denoted later in the code. In _Stata_ commenting within the code is done using \" * \"\n",
    "\n",
    "The first two cells are to call _Stata_ and to see if it actually works within the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipystata.config import config_stata\n",
    "config_stata('/Applications/Stata/StataSE.app/Contents/MacOS/StataSE') \n",
    "\n",
    "# Windows  --> 'C:\\Program Files (x86)\\Stata14\\StataSE-64.exe'\n",
    "# Mac OS X --> '/Applications/Stata/StataSE.app/Contents/MacOS/stataSE'\n",
    "# Linux    --> '/home/user/stata14/stata-se'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, I am printed by Stata\n",
      "We welcome you to our assignment and we hope you enjoy it\n"
     ]
    }
   ],
   "source": [
    "%%stata \n",
    "display \"Hello, I am printed by Stata\" \n",
    "display \"We welcome you to our assignment and we hope you enjoy it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/jakobschwerter/Dropbox/github/python/Stata/Assignment\n"
     ]
    }
   ],
   "source": [
    "%%stata \n",
    "cd \"/Users/jakobschwerter/Dropbox/github/python/Stata/Assignment\"\n",
    "    * cd \"D:\\Google Doc\\_Tilburg\\_Simulation\\stata\" \n",
    "set seed 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First we have to _call_ Stata using `%%stata`. Then by using `cd` we indicate where it should look for files and where it can save them.\n",
    "\n",
    "> We further set the seed to 100, so that results are easier to be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(obs 200)\n"
     ]
    }
   ],
   "source": [
    "%%stata -o simulation\n",
    "drawnorm alpha_i, n(200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We create a new dataset called _simulation_ using `-o`. The type is a pandas core frame Data Frame. This is necessary for `ipystata` on mac, because the data would not be stored otherwise.\n",
    "\n",
    ">Then we draw 200 (independent and identical distributed (iid) random numbers of a standard normal distribution with mean zero and variance 1. We name it $alpha_i$ ($\\alpha_i$). In the context of panel data, the variable $\\alpha_i$ stands for the unobserved time-invariant individual effect.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(800 observations created)\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation -o simulation\n",
    "expand 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> `-d` calls the dataset _simulation_ , otherwise the commands afterwards would not find any variables. The command `-o simulation` saves the changes.\n",
    "\n",
    "> We expand the generated data by 5, i.e. simply repeating every alpha 4 additional times. This creates 800 additional observations.\n",
    "\n",
    ">As a result, the vector expands from $(200 \\; \\text{x} \\; 1)$ to $(200 \\cdot 5 \\; \\text{x} \\; 1)$. What we have done is to create a variable of 800 observations, each described by some measure that is randomly generated. The expanding here ensures that for each individual the measure stays constant over time, thus replicating time-invariant individual effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation -o simulation\n",
    "drawnorm nu_it e_it, n(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We draw two new variables (or vectors) now with 1000 entries from a multivariate normal distribution. 1000 draws are made, so that this fits with the 200 persons and the 5 time periods we created by expanding the alphas by 5. Both variables are independent from each other, so they are not correlated, and the data is also i.i.d. over time.\n",
    " \n",
    "> To verify that we did everything correctly, we have a look into summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Variable |       Obs        Mean    Std. Dev.       Min        Max\n",
      "-------------+--------------------------------------------------------\n",
      "       index |      1000       499.5    288.8194          0        999\n",
      "     alpha_i |      1000    .0433381    .9505638  -1.962526   2.542606\n",
      "       nu_it |      1000      .06557    1.007816  -2.971299   3.528579\n",
      "        e_it |      1000   -.0280785    1.039311   -3.89692   3.124364\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation\n",
    "sum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We see the three generated variables $\\alpha_i, \\; nu_{it}, \\; e_{it}$, having a mean very close to zero and a standard deviation close to 1. Further, using stata in python, and index variable is generated automatically. The index variable would be the individual identifier variable if we would run a pooled panel regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation -o simulation\n",
    "g x_it = nu_it + alpha_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Generates our variable of interest $x_{it}$ using the two variables we created in a previous step. Since $\\alpha_i$ is part of it and is time-invariant, $x_{it}$ will be correlated over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation -o simulation\n",
    "g y_it=3+alpha_i+2*x_it+e_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Generates our dependent variable $y_{it}$. We use an intercept, $\\alpha_i$, dependent variable $x_{it}$ and residual $e_{it}$ for that. The assigned regression parameter to $x_{it}$ is $2$.\n",
    " \n",
    ">The underlying model is a panel data model, consisting of 200 individuals and 5 time periods per person. It is completely balanced. The variables $y_{it}$ and $x_{it}$ are correlated with a constant term $\\alpha_i$. This produces autocorrelation within an individual.\n",
    "\n",
    "\n",
    "\n",
    ">Checking if the code worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             |    index  alpha_i    nu_it     e_it     x_it     y_it\n",
      "-------------+------------------------------------------------------\n",
      "       index |   1.0000 \n",
      "             |\n",
      "             |\n",
      "     alpha_i |  -0.0304   1.0000 \n",
      "             |   0.3368\n",
      "             |\n",
      "       nu_it |  -0.0383  -0.0237   1.0000 \n",
      "             |   0.2258   0.4532\n",
      "             |\n",
      "        e_it |   0.0054   0.0121   0.0262   1.0000 \n",
      "             |   0.8646   0.7018   0.4079\n",
      "             |\n",
      "        x_it |  -0.0493   0.6769   0.7198   0.0277   1.0000 \n",
      "             |   0.1189   0.0000   0.0000   0.3814\n",
      "             |\n",
      "        y_it |  -0.0436   0.7757   0.5440   0.3103   0.9391   1.0000 \n",
      "             |   0.1682   0.0000   0.0000   0.0000   0.0000\n",
      "             |\n",
      "unrecognized command:  corrtex\n",
      "r(199);\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation\n",
    "pwcorr,sig\n",
    "corrtex alpha_i e_it x_it y_it, sig "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The command [_pwcorr_](http://www.stata.com/help.cgi?pwcorr) shows a correlation table of the variables in the dataset (which were generated previously). The option _sig_ prints significance level for each correlation. Results are presented above. There is a statistically significant correlation between $x_{it}$ and $\\alpha_i$ (77.64%). As a result, not including or solving for $\\alpha_i$ when regressing in a panel model $x_it$ on $y_{it}$ will produce biased estimates. The correlation of $x_{it}$ on $y_{it}$ and $\\alpha_i$ is naturally very high, since it depends on both variables in our set-up.\n",
    " The correlation of $e_{it}$ with $x_{it}$ and $\\alpha_i$ is very low and statistical insignificant, which is necessary and by construction. An OLS model like $y_{it} = \\beta x_{it}$  will be upward biased since the correlation of $\\alpha_i$ and $x_{it}$ is positive. $\\beta$ will estimate the effect of $\\alpha$ and $x_{it}$. If the correlation would be negative, it would have been downward biased.\n",
    "\n",
    " To check if the DGP is correct, we do one simulation without imposing a trend into the DGP.\n",
    "\n",
    "# 2.2. Simulation without Random Trend in the true DGP\n",
    "\n",
    "Now we run the DGP defined earlier and verify how different estimation procedures are able to capture the parameters we have embedded, namely a intercept of 3 and a $\\beta$ of 2. We evaluate the following panel data methods\n",
    "   + Pooled regression, i.e. ignoring the panel structure as presented in equation (1) and pooling time and individual information together \n",
    "   + Fixed Effects (FE), (2)\n",
    "   + First-Difference (FD), (3) \n",
    "   + Random-Trend using the first approach, namely taking two first-differences (_pure_), (4)\n",
    "   + Random-Trend using the second approach, namely taking first the  first-differences and then the fixed-effects (_mix_), (5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". qui expand 5\n",
      ". qui drawnorm nu_it e_it, n(1000)\n",
      ". qui xtset i t \n",
      ". qui gen dx_it = d.x_it \n",
      ". qui gen dy_it = d.y_it \n",
      ". . qui eststo: xtreg  y_it x_it, fe  \n",
      ". qui eststo: reg d.y_it d.x_it, \n",
      ". qui eststo: xtreg d.y_it d.x_it,fe \n",
      ". qui eststo: reg d2.y_it d2.x_it,  \n",
      ". esttab, long compress nogaps rename(D.x_it x_it D2.x_it x_it D.dx_it x_it) drop(_cons) ///\n",
      ">         b(%7,5f) se(%6.5f) scalars(\"N N\" \"F F-Stat\" \"p p-value\" \"r2 r2\") sfmt(%4,0f %5,2f %5,4f %6,5f %9,2f %9,2f) star(+ 0.1 * 0.05 ** 0.01 *** 0.001) ///\n",
      ">         label mtitles( \"OLS\" \"FE\" \"FD\" \"FD-FE\" \"2-FD\" \"FD-FD\" ) title(\"Without a random trend component\") nolabel replace\n",
      "\n",
      "Without a random trend component\n",
      "---------------------------------------------------------------------------------\n",
      "                       (1)          (2)          (3)          (4)          (5)   \n",
      "                       OLS           FE           FD        FD-FE         2-FD   \n",
      "---------------------------------------------------------------------------------\n",
      "x_it               2,50010***   2,05160***   2,00198***   1,98926***   1,99534***\n",
      "                 (0.03124)    (0.03384)    (0.03543)    (0.04120)    (0.04218)   \n",
      "---------------------------------------------------------------------------------\n",
      "Observations          1000         1000          800          800          600   \n",
      "F-Stat             6403,01      3675,01      3192,34      2331,34      2238,10   \n",
      "p-value             0,0000       0,0000       0,0000       0,0000       0,0000   \n",
      "r2                 0,89486      0,82141      0,80002      0,79559      0,78915   \n",
      "---------------------------------------------------------------------------------\n",
      "Standard errors in parentheses\n",
      "+ p<0.1, * p<0.05, ** p<0.01, *** p<0.001\n",
      ".     Time needed to run:\n",
      "   1:      0.46 /        1 =       0.4610\n"
     ]
    }
   ],
   "source": [
    "%%stata -o sim\n",
    "qui set seed 123\n",
    "qui drawnorm alpha_i, n(200)\n",
    "qui gen i = _n \n",
    "    * included additional to generate an index for the individual level\n",
    "qui expand 5\n",
    "qui bys i: g t = _n \n",
    "    * included additional to generate an index for the time level\n",
    "qui drawnorm nu_it e_it, n(1000)\n",
    "qui g x_it = nu_it + alpha_i\n",
    "qui g y_it = 3 + alpha_i + 2*x_it + e_it \n",
    "    * DGP\n",
    "\n",
    "qui xtset i t \n",
    "    * give stata panel-information\n",
    "\n",
    "qui gen dx_it = d.x_it \n",
    "    * generates the first-difference for x prior to the regression command\n",
    "qui gen dy_it = d.y_it \n",
    "    * generates the first-difference for y prior to the regression command\n",
    "\n",
    "\n",
    "qui eststo: reg y_it x_it, cluster(i) \n",
    "    * OLS-regression (biased) (eststo saves the regression results)\n",
    "qui eststo: xtreg  y_it x_it, fe  \n",
    "    * FE-regression (OLS, but first the within-transformation is done)\n",
    "qui eststo: reg d.y_it d.x_it, \n",
    "    * FD-regression\n",
    "qui eststo: xtreg d.y_it d.x_it,fe \n",
    "    * Random-Trend 1 (mix)\n",
    "qui eststo: reg d2.y_it d2.x_it,  \n",
    "    * Random-Trend 2 (pure)  \n",
    "\n",
    "esttab, long compress nogaps rename(D.x_it x_it D2.x_it x_it D.dx_it x_it) drop(_cons) ///\n",
    "\tb(%7,5f) se(%6.5f) scalars(\"N N\" \"F F-Stat\" \"p p-value\" \"r2 r2\") sfmt(%4,0f %5,2f %5,4f %6,5f %9,2f %9,2f) star(+ 0.1 * 0.05 ** 0.01 *** 0.001) ///\n",
    "\tlabel mtitles( \"OLS\" \"FE\" \"FD\" \"FD-FE\" \"2-FD\" \"FD-FD\" ) title(\"Without a random trend component\") nolabel replace\n",
    "    * esttab produces the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that the results of the normal OLS are biased, i.e. the coefficient for $x_{it}$ is not 2 but around 2.5. For FE and the FD however the coefficient if very close to 2.\n",
    "Also both random-trend (RT) approaches are unbiased. _mix_ is farthermost of the true parameter and also the standard error is the highest (together with the other RT-approach). So we can conclude that even if we do not have a random-trend component, using two transformations does not really harm the estimation.\n",
    "\n",
    "> Comparing the different RT calculations, we see that the _mix_ seem to be more efficient, but the _pure_ is closer to the true value. This is consistent comparing just the normal FE and FD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Monte Carlo Study without Trend in the DGP\n",
    "\n",
    "In a single simulation we need to keep in mind that the results from just one run can be misleading, as they can be driven by chance if the seed in use simply draws numbers which work better for one method than the other. Therefore, we will run a program now to do a Monte Carlo study which will take 1000 replications to see if above results are robust.\n",
    "\n",
    "What we add in the code now is:\n",
    "\n",
    "   1. A timer to measure the time the code runs. The necessary commands are `timer on 1`, starting the timer, called '1', `timer off 1`, stopping the started timer and `timer list 1`, displaying the time needed.\n",
    "   2.  For the simulation commands `simulation`, we first have to embed the code in a program. The first line `capture program drop mcprog_pool` drops if somehow the program is already defined. This command is useful if you run the code an additional time. `program mcprog_pool` starts the program and ends with `end`.\n",
    "\n",
    "The DGP in each of the following simulations stays the same, namely\n",
    "\n",
    "$$y_{it} = 3 + \\alpha_i + 2\\cdot x_{it} + e_{it} $$\n",
    "\n",
    "What we change in each of the Monte Carlo specifications is the estimation procedure by applying each of the procedures discussed so far. In the code, the following notation will be used: _pool_ stands for the pooled regress, _fe_ for the within-transformation, _fd_ for the first-difference, _mix_ for the random-trend, using first-difference and fixed-effects and _pure_ the random trend with two first-differences.  \n",
    "\n",
    "The simulation results are stored in `_b` for the coefficient and `_se` for the standard errors. <span style=\"color: green\">The simulation of the standard errors differ from the standard deviation of the estimate ($\\beta$) which was presented in the table above. In the simulation only the variation in beta is used. It is not calculated\n",
    "by the normal variance formula which includes the variances of the error component $\\alpha_{i}$ und $e_{it}$. </span>\n",
    "\n",
    "We take 1000 replications and we call the earlier defined program `mcprog_pool` (Monte Carlo Program - Pooled Regression). \n",
    "\n",
    "At the end we rename the coefficient for a better understanding and drop everything else from the data frame (`keep beta_pool`).\n",
    "\n",
    "\n",
    "* ###  Pooled OLS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:     11.69 /        1 =      11.6920\n"
     ]
    }
   ],
   "source": [
    "%%stata -o pool \n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pool\n",
    "program mcprog_pool\n",
    "         clear\n",
    "         drawnorm alpha_i, n(200)\n",
    "         gen i = _n \n",
    "         expand 5\n",
    "         bys i: g t = _n \n",
    "         drawnorm nu_it e_it, n(1000)\n",
    "         g x_it=nu_it+alpha_i\n",
    "         g y_it=3+alpha_i+2*x_it+e_it\n",
    "         reg y_it x_it, \n",
    "         drop nu_it alpha_i e_it y_it x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(1000): mcprog_pool\n",
    "        rename _b_x_it beta_pool\n",
    "        rename _se_x_it se_pool\n",
    "        keep beta_pool se_pool\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ###  FE Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:     58.52 /        1 =      58.5190\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fe\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "         clear\n",
    "         drawnorm alpha_i, n(200)\n",
    "         gen i = _n \n",
    "         expand 5\n",
    "         bys i: g t = _n \n",
    "         drawnorm nu_it e_it, n(1000)\n",
    "         g x_it=nu_it+alpha_i\n",
    "         g y_it=3+alpha_i+2*x_it+e_it\n",
    "         xtset i t\n",
    "         xtreg y_it x_it, fe \n",
    "         drop nu_it alpha_i e_it y_it x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(1000): mcprog_fe\n",
    "        rename _b_x_it beta_fe\n",
    "        rename _se_x_it se_fe\n",
    "        keep beta_fe se_fe\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ###  FD Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:     20.15 /        1 =      20.1540\n",
      ".         \n"
     ]
    }
   ],
   "source": [
    "%%stata -o fd\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fd\n",
    "        program mcprog_fd\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it=nu_it+alpha_i\n",
    "        g y_it=3+alpha_i+2*x_it+e_it\n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it, \n",
    "        drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(1000): mcprog_fd\n",
    "        rename _b_dx_it beta_fd\n",
    "        rename _se_dx_it se_fd\n",
    "        keep beta_fd se_fd\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ###  FD-FE (mix) Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:     72.02 /        1 =      72.0190\n"
     ]
    }
   ],
   "source": [
    "%%stata -o mix\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_mix\n",
    "        program mcprog_mix\n",
    "         clear\n",
    "         drawnorm alpha_i, n(200)\n",
    "         gen i = _n \n",
    "         expand 5\n",
    "         bys i: g t = _n \n",
    "         drawnorm nu_it e_it, n(1000)\n",
    "         g x_it=nu_it+alpha_i\n",
    "        g y_it=3+alpha_i+2*x_it+e_it\n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "         xtreg dy_it dx_it, fe \n",
    "         drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(1000): mcprog_mix\n",
    "        rename _b_dx_it beta_mix\n",
    "        rename _se_dx_it se_mix\n",
    "        keep beta_mix se_mix\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ###  FD-FD (pure) Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:     18.99 /        1 =      18.9920\n"
     ]
    }
   ],
   "source": [
    "%%stata -o pure\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pure\n",
    "        program mcprog_pure\n",
    "         clear\n",
    "         drawnorm alpha_i, n(200)\n",
    "         gen i = _n \n",
    "         expand 5\n",
    "         bys i: g t = _n \n",
    "         drawnorm nu_it e_it, n(1000)\n",
    "         g x_it=nu_it+alpha_i\n",
    "         g y_it=3+alpha_i+2*x_it+e_it\n",
    "        xtset i t\n",
    "        g d2x_it = d2.x_it\n",
    "        g d2y_it = d2.y_it\n",
    "         reg d2y_it d2x_it, \n",
    "         drop nu_it alpha_i e_it y_it x_it d2y_it d2x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(1000): mcprog_pure\n",
    "        rename _b_d2x_it beta_pure\n",
    "        rename _se_d2x_it se_pure\n",
    "        keep beta_pure se_pure\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, before we talk about the regression results, we want to point out that the within-transformation in FE and _mix_ is the reason the time is much higher in those to simulations. Thus, those are less efficient in the sense on how fast they can be calculated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* ###  Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OLS        FE        FD       mix      pure\n",
      "beta  2.499105  2.001513  2.000731  2.000577  1.999209\n",
      "se    0.027386  0.035339  0.035368  0.040823  0.040857\n"
     ]
    }
   ],
   "source": [
    "############################ Regression Results without trend\n",
    "# Pool Regression Results\n",
    "pool_mean = pool.mean() # Storing the results produces by the pool-simulation\n",
    "beta_pool = pool_mean[0] # Results of the coefficient\n",
    "se_pool = pool_mean[1] # Results of the standard error\n",
    "# FE Regression Results\n",
    "fe_mean = fe.mean()\n",
    "beta_fe = fe_mean[0]\n",
    "se_fe = fe_mean[1]\n",
    "# FD Regression Results\n",
    "fd_mean = fd.mean()\n",
    "beta_fd = fd_mean[0]\n",
    "se_fd = fd_mean[1]\n",
    "# Mix Regression Results\n",
    "mix_mean = mix.mean()\n",
    "beta_mix = mix_mean[0]\n",
    "se_mix = mix_mean[1]\n",
    "# Pure Regression Results\n",
    "pure_mean = pure.mean()\n",
    "beta_pure = pure_mean[0]\n",
    "se_pure = pure_mean[1]\n",
    "\n",
    "\n",
    "results_notrend_beta = np.array([beta_pool, beta_fe, beta_fd, beta_mix, beta_pure])\n",
    "results_notrend_se = np.array([se_pool, se_fe, se_fd, se_mix, se_pure])\n",
    "results_notrend = np.mat([results_notrend_beta, results_notrend_se]) \n",
    "results_notrend = pd.DataFrame(results_notrend)\n",
    "results_notrend.index = [\"Beta\", \"SE\"]\n",
    "results_notrend.columns = [\"Pool\", \"FE\", \"FD\", \"mix\", \"pure\"]\n",
    "print results_notrend\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that the pooled regression is upward biased from the true beta value (2). This is in line with econometric theory, as not considering the nature of the panel and pooling together the unobserved individual effects produces biased results. All other methods produce unbiased results that very close to each other. The SE are smallest for the pooled regression. Then the FE and FD are very close to each other and both RT has the highest variation. For every transformation we do we have a loss in efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2.4. Monte Carlo Study with the same trend for all individuals\n",
    "\n",
    "Here we investigate what happens to our estimator as we include the same trend for all individuals. If we would just run normal regression as presented first, we could just use the same datasets, replacing just some variables. But since we have to define programs, we need to copy all the programs and change for the trend. We will leave out the pooled regression because we already know, that this one is biased just due to the constant $\\alpha_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      6.10 /        1 =       6.0970\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fe_st\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + t \n",
    "            *NOW: x_it is correlated with a linear term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + t + e_it \n",
    "            *DGP includes know the linear trend\n",
    "        xtset i t\n",
    "        xtreg y_it x_it, fe \n",
    "        drop nu_it alpha_i e_it y_it x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_x_it beta_fe_st\n",
    "        rename _se_x_it se_fe_st\n",
    "        keep beta_fe_st se_fe_st\n",
    "            *st = same trend\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:      1.87 /        1 =       1.8670\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fd_st\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + t \n",
    "            * NOW: x_it is correlated with a linear term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + t + e_it \n",
    "            * DGP includes know the linear trend\n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it, \n",
    "        drop nu_it alpha_i e_it y_it x_it dy_it dx_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_dx_it beta_fd_st\n",
    "        rename _se_dx_it se_fd_st\n",
    "        keep beta_fd_st se_fd_st\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      5.84 /        1 =       5.8430\n"
     ]
    }
   ],
   "source": [
    "%%stata -o mix_st\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_mix\n",
    "        program mcprog_mix\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + t \n",
    "            * NOW: x_it is correlated with a linear term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + t + e_it \n",
    "            * DGP includes know the linear trend\n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        xtreg dy_it dx_it, fe \n",
    "        drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_mix\n",
    "        rename _b_dx_it beta_mix_st\n",
    "        rename _se_dx_it se_mix_st\n",
    "        keep beta_mix_st se_mix_st\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      1.97 /        1 =       1.9660\n"
     ]
    }
   ],
   "source": [
    "%%stata -o pure_st\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pure\n",
    "        program mcprog_pure\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + t \n",
    "            * NOW: x_it is correlated with a linear term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + t + e_it \n",
    "            * DGP includes know the linear trend\n",
    "        xtset i t\n",
    "        g d2x_it = d2.x_it\n",
    "        g d2y_it = d2.y_it\n",
    "        reg d2y_it d2x_it, \n",
    "            * cluster(i)\n",
    "        drop nu_it alpha_i e_it y_it x_it d2y_it d2x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_pure\n",
    "        rename _b_d2x_it beta_pure_st\n",
    "        rename _se_d2x_it se_pure_st\n",
    "        keep beta_pure_st se_pure_st\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            FE        FD       mix      pure\n",
      "Beta  2.714095  2.003978  2.004183  2.002594\n",
      "SE    0.024645  0.035522  0.040986  0.041070\n"
     ]
    }
   ],
   "source": [
    "############################ Regression Results with same trend\n",
    "# FE Regression Results\n",
    "fe_mean = fe_st.mean()\n",
    "beta_fe = fe_mean[0]\n",
    "se_fe = fe_mean[1]\n",
    "# FD Regression Results\n",
    "fd_mean = fd_st.mean()\n",
    "beta_fd = fd_mean[0]\n",
    "se_fd = fd_mean[1]\n",
    "# Mix Regression Results\n",
    "mix_mean = mix_st.mean()\n",
    "beta_mix = mix_mean[0]\n",
    "se_mix = mix_mean[1]\n",
    "# Pure Regression Results\n",
    "pure_mean = pure_st.mean()\n",
    "beta_pure = pure_mean[0]\n",
    "se_pure = pure_mean[1]\n",
    "\n",
    "\n",
    "results_sametrend_beta = np.array([beta_fe, beta_fd, beta_mix_st, beta_pure])\n",
    "results_sametrend_se = np.array([se_fe, se_fd, se_mix_st, se_pure])\n",
    "results_sametrend = np.mat([results_sametrend_beta, results_sametrend_se]) \n",
    "results_sametrend = pd.DataFrame(results_sametrend)\n",
    "results_sametrend.index = [\"Beta\", \"SE\"]\n",
    "results_sametrend.columns = [ \"FE\", \"FD\", \"mix\", \"pure\"]\n",
    "print results_sametrend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> As expected, the simulation shows that if the trend is the same for all individuals in the data, the within-transformation cannot solve the linear trend. Thereby the coefficient is biased. Surprisingly however, the first-difference is enough to solve for the problem introduced by a correlation of the variable of interest and the trend. We will not take a deeper look here, but it might be interest to investigate how many individuals are allowed to differ so that the first-difference is not unbiased anymore.\n",
    "The FD is thereby more robust than the FE. It is however questionable in empirical work if the observed individuals really follow the same trend.\n",
    "\n",
    ">Further, the first-difference performs better than the _mix_, but worse than the _pure_. The _pure_ again has a higher standard error than the _mix_.\n",
    "\n",
    "# 2.5. Monte Carlo study with individual specific linear trends (Random Trend)\n",
    "\n",
    "\n",
    "<span style=\"color: green\"> Here we will (finally) introduce the model including the individual specific linear trend. Here we expect that only the two RT-methods will solve for the bias in $x_it$ and that now both, FE and FD preform poorly. </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      5.78 /        1 =       5.7810\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fe_it\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = t*i\n",
    "            * individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "            * NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        xtreg y_it x_it, fe \n",
    "            * cluster(i)\n",
    "        drop nu_it alpha_i e_it y_it x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_x_it beta_fe_it\n",
    "        rename _se_x_it se_fe_it\n",
    "        keep beta_fe_it se_fe_it\n",
    "            * it = individual trend\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:      1.96 /        1 =       1.9620\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fd_it\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = t*i \n",
    "            * individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "            * NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it, \n",
    "        drop nu_it alpha_i e_it y_it x_it dy_it dx_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_dx_it beta_fd_it\n",
    "        rename _se_dx_it se_fd_it\n",
    "        keep beta_fd_it se_fd_it\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      6.00 /        1 =       6.0010\n"
     ]
    }
   ],
   "source": [
    "%%stata -o mix_it\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_mix\n",
    "        program mcprog_mix\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n\n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = t*i \n",
    "            * individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "            * NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        xtreg dy_it dx_it, fe \n",
    "        drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_mix\n",
    "        rename _b_dx_it beta_mix_it\n",
    "        rename _se_dx_it se_mix_it\n",
    "        keep beta_mix_it se_mix_it\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      1.96 /        1 =       1.9550\n"
     ]
    }
   ],
   "source": [
    "%%stata -o pure_it\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pure\n",
    "        program mcprog_pure\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = t*i \n",
    "            * individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "            * NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g d2x_it = d2.x_it\n",
    "        g d2y_it = d2.y_it\n",
    "        reg d2y_it d2x_it, \n",
    "        drop nu_it alpha_i e_it y_it x_it d2y_it d2x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_pure\n",
    "        rename _b_d2x_it beta_pure_it\n",
    "        rename _se_d2x_it se_pure_it\n",
    "        keep beta_pure_it se_pure_it\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            FE        FD       mix      pure\n",
      "Beta  2.999957  2.999481  2.004183  2.002594\n",
      "SE    0.000272  0.001220  0.040986  0.041070\n"
     ]
    }
   ],
   "source": [
    "############################ Regression Results with individual specific linear trend\n",
    "# FE Regression Results\n",
    "fe_mean_it = fe_it.mean()\n",
    "beta_fe_it = fe_mean_it[0]\n",
    "se_fe_it = fe_mean_it[1]\n",
    "# FD Regression Results\n",
    "fd_mean_it = fd_it.mean()\n",
    "beta_fd_it = fd_mean_it[0]\n",
    "se_fd_it = fd_mean_it[1]\n",
    "# Mix Regression Results\n",
    "mix_mean_it = mix_it.mean()\n",
    "beta_mix_it = mix_mean_it[0]\n",
    "se_mix_it = mix_mean_it[1]\n",
    "# Pure Regression Results\n",
    "pure_mean_it = pure_it.mean()\n",
    "beta_pure_it = pure_mean_it[0]\n",
    "se_pure_it = pure_mean_it[1]\n",
    "\n",
    "\n",
    "results_inditrend_beta = np.array([beta_fe_it, beta_fd_it, beta_mix_it, beta_pure_it])\n",
    "results_inditrend_se = np.array([se_fe_it, se_fd_it, se_mix_it, se_pure_it])\n",
    "results_inditrend = np.mat([results_inditrend_beta, results_inditrend_se]) \n",
    "results_inditrend = pd.DataFrame(results_inditrend)\n",
    "results_inditrend.index = [\"Beta\", \"SE\"]\n",
    "results_inditrend.columns = [ \"FE\", \"FD\", \"mix\", \"pure\"]\n",
    "print results_inditrend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The results confirm (unsurprisingly) the bad results of the FE. Now, also the FD is not capable to control for the trend within $x_{it}$. Only unbiased estimations are obtained by _mix_ and _pure_. The results are the same as before (remember: same seed and the only difference is canceled out). After checking that we really changed the code, it can be concluded that for the random-trend model it does not matter whether the trend is constant or individual specific.\n",
    "\n",
    "> In this easy example it further seems that the double difference (_pure_) is more robust, i.e. closer to the true value. Thereby the example is in favor to use the _pure_ rather than the _mix_ random-trend method. Also the time point, the _pure_ is much faster. Only the standard error suggest to use _mix_.\n",
    "\n",
    "# 2.6. Monte Carlo study with nonlinear individual specific trends \n",
    "\n",
    "We will further check for two cases in which the trend is non-linear. The motivation is to see if one of the random-trend methods performs better even though the baseline assumption of a linear trend does not hold. Therefore we will use an exponential individual specific trend and a log individual specific trend. \n",
    "\n",
    "Here we will not include the FE estimation anymore, because it preforms equally bad or worse than the FD. The FD is included to check whether the RT are of additional help.\n",
    "\n",
    "### 2.6.1 Monte Carlo study with exponential individual specific trends \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:      2.12 /        1 =       2.1170\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fd_exp\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = exp(t*i/100) \n",
    "            * exponential individual specific trand   \n",
    "            * We devide by 100, because otherwise the values of the trend would have been to high at the end\n",
    "            * Thereby it would have dominated the whole term, in our case resulting in no variation between the replications\n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it, \n",
    "        drop nu_it alpha_i e_it y_it x_it dy_it dx_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_dx_it beta_fd_exp\n",
    "        rename _se_dx_it se_fd_exp\n",
    "        keep beta_fd_exp se_fd_exp\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      6.43 /        1 =       6.4310\n"
     ]
    }
   ],
   "source": [
    "%%stata -o mix_exp\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_mix\n",
    "        program mcprog_mix\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = exp(t*i/100) \n",
    "            * exponential individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        xtreg dy_it dx_it, fe \n",
    "        drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_mix\n",
    "        rename _b_dx_it beta_mix_exp\n",
    "        rename _se_dx_it se_mix_exp\n",
    "        keep beta_mix_exp se_mix_exp\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ".         The time this codes run is\n",
      "   1:      2.67 /        1 =       2.6670\n"
     ]
    }
   ],
   "source": [
    "%%stata -o pure_exp\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pure\n",
    "        program mcprog_pure\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = exp(t*i/100) \n",
    "            * exponential individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g d2x_it = d2.x_it\n",
    "        g d2y_it = d2.y_it\n",
    "        reg d2y_it d2x_it, \n",
    "        drop nu_it alpha_i e_it y_it x_it d2y_it d2x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_pure\n",
    "        rename _b_d2x_it beta_pure_exp\n",
    "        rename _se_d2x_it se_pure_exp\n",
    "        keep beta_pure_exp se_pure_exp\n",
    "            * exp for exponential\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            FD       mix      pure\n",
      "Beta  3.000002  3.000002  2.999999\n",
      "SE    0.000034  0.000045  0.000069\n"
     ]
    }
   ],
   "source": [
    "############################ Regression Results with exponential individual specific linear trend\n",
    "# FD Regression Results\n",
    "fd_mean_exp = fd_exp.mean()\n",
    "beta_fd_exp = fd_mean_exp[0]\n",
    "se_fd_exp = fd_mean_exp[1]\n",
    "# Mix Regression Results\n",
    "mix_mean_exp = mix_exp.mean()\n",
    "beta_mix_exp = mix_mean_exp[0]\n",
    "se_mix_exp = mix_mean_exp[1]\n",
    "# Pure Regression Results\n",
    "pure_mean_exp = pure_exp.mean()\n",
    "beta_pure_exp = pure_mean_exp[0]\n",
    "se_pure_exp = pure_mean_exp[1]\n",
    "\n",
    "\n",
    "results_exptrend_beta = np.array([beta_fd_exp, beta_mix_exp, beta_pure_exp])\n",
    "results_exptrend_se = np.array([se_fd_exp, se_mix_exp, se_pure_exp])\n",
    "results_exptrend = np.mat([results_exptrend_beta, results_exptrend_se]) \n",
    "results_exptrend = pd.DataFrame(results_exptrend)\n",
    "results_exptrend.index = [\"Beta\", \"SE\"]\n",
    "results_exptrend.columns = [\"FD\", \"mix\", \"pure\"]\n",
    "print results_exptrend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we have the case that the trend increases individual specific exponential over time, i.e. the trend is a convex function.\n",
    "Both RT methods are biased, and the difference between the two is at the 6. digit, so obsolete from our point of view. Surprisingly to us is, that there is no difference between the normal FD and the RT's. This means that if the trend increases exponential, the random trend method does not improve the estimation. I would have expected that the random-trend model comes a bite closer to the true, but in this example it does not. Thus, we just get higher standard errors without the trade of more robust results.\n",
    "\n",
    "> ### 2.6.2 Monte Carlo study with log individual specific trends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:      2.30 /        1 =       2.3030\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fd_log\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = log(t*i) \n",
    "            * logarithm !!! \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it\n",
    "        drop nu_it alpha_i e_it y_it x_it dy_it dx_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_dx_it beta_fd_log\n",
    "        rename _se_dx_it se_fd_log\n",
    "        keep beta_fd_log se_fd_log\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      7.34 /        1 =       7.3440\n"
     ]
    }
   ],
   "source": [
    "%%stata -o mix_log\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_mix\n",
    "        program mcprog_mix\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = log(t*i) \n",
    "            * logarithm!\n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        xtreg dy_it dx_it, fe \n",
    "            * cluster(i)\n",
    "        drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_mix\n",
    "        rename _b_dx_it beta_mix_log\n",
    "        rename _se_dx_it se_mix_log\n",
    "        keep beta_mix_log se_mix_log\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      2.19 /        1 =       2.1950\n"
     ]
    }
   ],
   "source": [
    "%%stata -o pure_log\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pure\n",
    "        program mcprog_pure\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = log(t*i) \n",
    "            * logarithm! \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g d2x_it = d2.x_it\n",
    "        g d2y_it = d2.y_it\n",
    "        reg d2y_it d2x_it, \n",
    "        drop nu_it alpha_i e_it y_it x_it d2y_it d2x_it\n",
    "end\n",
    "        simulate _b _se, nodots nolegend  reps(100): mcprog_pure\n",
    "        rename _b_d2x_it beta_pure_log\n",
    "        rename _se_d2x_it se_pure_log\n",
    "        keep beta_pure_log se_pure_log\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            FD       mix      pure\n",
      "Beta  2.020059  2.021325  2.004355\n",
      "SE    0.035496  0.040955  0.041070\n"
     ]
    }
   ],
   "source": [
    "############################ Regression Results with log individual specific linear trend\n",
    "# FD Regression Results\n",
    "fd_mean_log = fd_log.mean()\n",
    "beta_fd_log = fd_mean_log[0]\n",
    "se_fd_log = fd_mean_log[1]\n",
    "# Mix Regression Results\n",
    "mix_mean_log = mix_log.mean()\n",
    "beta_mix_log = mix_mean_log[0]\n",
    "se_mix_log = mix_mean_log[1]\n",
    "# Pure Regression Results\n",
    "pure_mean_log = pure_log.mean()\n",
    "beta_pure_log = pure_mean_log[0]\n",
    "se_pure_log = pure_mean_log[1]\n",
    "\n",
    "\n",
    "results_logtrend_beta = np.array([beta_fd_log, beta_mix_log, beta_pure_log])\n",
    "results_logtrend_se = np.array([se_fd_log, se_mix_log, se_pure_log])\n",
    "results_logtrend = np.mat([results_logtrend_beta, results_logtrend_se]) \n",
    "results_logtrend = pd.DataFrame(results_logtrend)\n",
    "results_logtrend.index = [\"Beta\", \"SE\"]\n",
    "results_logtrend.columns = [\"FD\", \"mix\", \"pure\"]\n",
    "print results_logtrend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the log individual specific trend we have the case that the trend is upward sloping, but the marginal increase decreases, i.e. we have concave trends.\n",
    "All three methods are good in capturing the _unobserved_ trend component. As for the constant trend, the _mix_ performs worst, FD second and _pure_ performs best.\n",
    "\n",
    "> Again, FD is capable to solve the endogeneity problem and there is no need for a random-trend model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The not-working within-transformation first-step: \n",
    "At the very top we stated that it does not make any sense to first transform the data using the within-transformation and then using the first-difference. For the canceling of the trend, it was crucial to take first the first-difference. We will investigate this in the following using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      6.65 /        1 =       6.6460\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fe_fe\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = t*i  \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        xtdata i t x_it y_it, fe clear \n",
    "            * preforms the within-transformation, so we can be sure that it is really the first thing in the regression\n",
    "        xtreg y_it x_it, fe \n",
    "            *  Random-Trend (FE-FD)\n",
    "        drop y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_x_it beta_fe_fe\n",
    "        keep beta_fe_fe \n",
    "            * it = individual trend\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      4.11 /        1 =       4.1140\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fe_fd\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = t*i  \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        xtdata i t x_it y_it, fe clear \n",
    "            * preforms the within-transformation, so we can be sure that it is really the first thing in the regression\n",
    "        xtset i t \n",
    "            * to sort the data\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it,  \n",
    "            * Random-Trend (FE-FD)\n",
    "        drop y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_dx_it beta_fe_fd\n",
    "        keep beta_fe_fd \n",
    "            * it = individual trend\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Beta\n",
      "FE-FE  2.999957\n",
      "FE-FD  2.999481\n"
     ]
    }
   ],
   "source": [
    "fe_fe_mean = fe_fe.mean()\n",
    "fe_fd_mean = fe_fd.mean()\n",
    "\n",
    "results_within_beta = np.mat([fe_fe_mean, fe_fd_mean])\n",
    "results_within = pd.DataFrame(results_within_beta)\n",
    "results_within.index = [\"FE-FE\", \"FE-FD\"]\n",
    "results_within.columns = [\"Beta\"]\n",
    "print results_within\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Results are similar biased to the normal FE. Doing the second transformation is not helpful at all, i.e. the results confirm the theoretical model. We suppress the standard errors because the results are inconsistent anyway and are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Trend Simulation using Python:\n",
    "Since the course is not just about doing a simulation within Jupyter Notebook, but learning free software packages (especially python), we will now do a similar simulation using Python. \n",
    "\n",
    "As before, we will first do an example with just one simulation To whether the code works or not. In a second step, we will run the same simulation with 1000 replication to see how the codes behaves.\n",
    "\n",
    "Concluding from our benchmark results, we will only look into the case of the linear individual specific trend. If we get similar results here, results should hold for the others as well. Further, this case is our main question of interest.\n",
    "\n",
    "There are some packages which can be used to run regressions, but not package explicit for panel-data transformation. Since we had to write the transformation on our own, we decided calculate the regression ourselves as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Individual  Time   alpha_i  trend         y_it         x_it      e_it\n",
      "0             1     1  0.593090      1     8.108892     2.082130 -0.648458\n",
      "1             1     2  0.593090      2    12.019683     2.983219  0.460155\n",
      "2             1     3  0.593090      3    11.904994     3.312496 -1.313087\n",
      "3             1     4  0.593090      4    16.348122     4.754647 -0.754261\n",
      "4             1     5  0.593090      5    20.655997     6.347359 -0.631809\n",
      "5             2     1 -0.417372      2     3.891319    -0.231898 -0.227513\n",
      "6             2     2 -0.417372      4    11.806292     2.365821  0.492021\n",
      "7             2     3 -0.417372      6    20.439948     5.098235  1.660849\n",
      "8             2     4 -0.417372      8    24.816812     6.134033  1.966117\n",
      "9             2     5 -0.417372     10    29.612286     8.706628 -0.383597\n",
      "10            3     1 -0.028673      3    14.256598     4.111380  0.062510\n",
      "11            3     2 -0.028673      6    18.932003     5.742992 -1.525308\n",
      "12            3     3 -0.028673      9    33.849486    10.317170  1.243820\n",
      "13            3     4 -0.028673     12    38.274202    11.296444  0.709987\n",
      "14            3     5 -0.028673     15    46.520725    14.956458 -1.363518\n",
      "15            4     1 -0.030502      4    11.245388     2.508046 -0.740202\n",
      "16            4     2 -0.030502      8    26.015815     7.572796 -0.099275\n",
      "17            4     3 -0.030502     12    37.875955    12.074385 -1.242313\n",
      "18            4     4 -0.030502     16    50.536093    16.871475 -2.176354\n",
      "19            4     5 -0.030502     20    60.442784    19.074224 -0.675162\n",
      "20            5     1 -0.809331      5    15.440698     4.220545 -0.191061\n",
      "21            5     2 -0.809331     10    34.315197    10.552477  1.019574\n",
      "22            5     3 -0.809331     15    44.861340    14.362828 -1.054985\n",
      "23            5     4 -0.809331     20    58.927411    18.088462  0.559819\n",
      "24            5     5 -0.809331     25    74.543043    24.006066 -0.659757\n",
      "25            6     1  0.754214      6    22.687198     6.165851  0.601282\n",
      "26            6     2  0.754214     12    39.049387    11.935625 -0.576078\n",
      "27            6     3  0.754214     18    55.693121    17.241942 -0.544977\n",
      "28            6     4  0.754214     24    77.902353    24.647747  0.852645\n",
      "29            6     5  0.754214     30    94.996821    30.456089  0.330428\n",
      "..          ...   ...       ...    ...          ...          ...       ...\n",
      "970         195     1 -0.813166    195   586.106500   194.576288 -0.232911\n",
      "971         195     2 -0.813166    390  1169.766926   388.181830  1.216432\n",
      "972         195     3 -0.813166    585  1759.563910   586.094756  0.187563\n",
      "973         195     4 -0.813166    780  2341.197302   779.416670  0.177128\n",
      "974         195     5 -0.813166    975  2926.314423   974.406009  0.315571\n",
      "975         196     1  0.943684    196   588.568996   195.342359 -2.059406\n",
      "976         196     2  0.943684    392  1183.475403   392.919035  1.693649\n",
      "977         196     3  0.943684    588  1770.285796   588.843149  0.655815\n",
      "978         196     4  0.943684    784  2356.436295   783.917648  0.657315\n",
      "979         196     5  0.943684    980  2944.789151   980.698434 -0.551401\n",
      "980         197     1  2.300152    197   599.657792   199.190945 -1.024251\n",
      "981         197     2  2.300152    394  1192.114477   396.906419 -0.998513\n",
      "982         197     3  2.300152    591  1776.144336   590.121955 -0.399727\n",
      "983         197     4  2.300152    788  2374.208243   790.477352 -0.046612\n",
      "984         197     5  2.300152    985  2964.963972   986.426847  1.810125\n",
      "985         198     1  0.406257    198   600.588609   199.101443  0.979466\n",
      "986         198     2  0.406257    396  1191.667435   396.824483 -1.387788\n",
      "987         198     3  0.406257    594  1784.757475   594.061250 -0.771282\n",
      "988         198     4  0.406257    792  2378.034744   791.343394 -0.058301\n",
      "989         198     5  0.406257    990  2975.483280   991.548533 -1.020043\n",
      "990         199     1  1.255823    199   603.061334   199.441300  0.922910\n",
      "991         199     2  1.255823    398  1203.259403   400.660031 -0.316482\n",
      "992         199     3  1.255823    597  1795.859330   596.711052  1.181402\n",
      "993         199     4  1.255823    796  2393.677189   796.561818  0.297730\n",
      "994         199     5  1.255823    995  2993.063064   996.242473  1.322296\n",
      "995         200     1  1.447307    200   608.317675   201.360252  1.149863\n",
      "996         200     2  1.447307    400  1208.315206   401.998558 -0.129217\n",
      "997         200     3  1.447307    600  1806.294744   600.778513  0.290411\n",
      "998         200     4  1.447307    800  2407.986492   801.301768  0.935648\n",
      "999         200     5  1.447307   1000  3008.095522  1000.950054  1.748107\n",
      "\n",
      "[1000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "random.seed(100) # Equivalent to `set seed 100`\n",
    "############################################### Somehow the seed does not work\n",
    "\n",
    "# For the coding part, we will set some variables which was not necessary in stata:\n",
    "n = 200 # in python there is no need (or not allowed) to write `generate`\n",
    "T = 5\n",
    "N = n * T\n",
    "\n",
    "alpha_i = np.random.normal(0, 1, 200) # Equivalent to `drawnorm alpha_i, n(200)`\n",
    "alpha_i = np.repeat(alpha_i,5) # Equivalent to `expand 5`  \n",
    "\n",
    "i = np.arange(n) + 1 # Equivalent to `gen i = _n`  # +1 because python would start with 0\n",
    "i = np.repeat(i,T) # Equivalent to `expand 5`  \n",
    "\n",
    "t = np.arange(5) + 1 # Equivalent to bys i: g t = _n \n",
    "t = np.tile(t,200) # Equivalent to `expand 5`  \n",
    "\n",
    "trend = t*i # <=> `gen trend = t*i`\n",
    "\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "nu_it, e_it = np.random.multivariate_normal(mean, cov, 1000).T # <=> `drawnorm nu_it e_it, n(1000)`\n",
    "\n",
    "x_it = (nu_it + alpha_i + trend).T # <=> `g x_it = nu_it + alpha_i + trend`\n",
    "# Since we will transform the data at least ones, we don't have to include an intercept in the first place.\n",
    "\n",
    "y_it = (3 + alpha_i + 2*x_it + trend + e_it).T # <=> `g y_it = 3 + alpha_i + 2*x_it + trend + e_it `\n",
    "\n",
    "m = np.matrix((i, t, alpha_i, trend, y_it, x_it, e_it))\n",
    "mt = m.transpose()\n",
    "df = pd.DataFrame(mt)\n",
    "df.columns = [\"Individual\", \"Time\", \"alpha_i\", \"trend\", \"y_it\", \"x_it\", \"e_it\"]\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The data generating process is here now down completely using open source packages (numpy). We print the data frame to have a quick overview whether the the DGP was correct or not. Judging from the output, it seems that so far everything is fine.\n",
    "\n",
    "> The arising problem now however is, that our Internet research did not provide us with commands which can analyze panel data as easily as Stata can do. I.e. there is no command which is directly comparable to \n",
    "`xtset i t` and `xtreg y_it x_it, fe`. We therefor have to construct the transformations ourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# First and double Difference for x:\n",
    "x_reshape = np.reshape(np.ravel(x_it), (200, 5)).T # reshape x-variable to wide form for the transformation\n",
    "    # To verify the reshape command, comment out the next to lines. We need 200 columsn with each 5 rows for 200 individuals haven 5 time periods\n",
    "    #x_df = pd.DataFrame(x_reshape)\n",
    "    #print x_df.head(n=5) # One can see that we have now 200 columns with each 5 time periods\n",
    "dx = np.zeros(x_reshape.shape) #Constructing a zero variable for the first-difference command which is following (this is necessary)\n",
    "dx[1:] = x_reshape[1:] - x_reshape[:-1] # First-Difference\n",
    "dx = np.delete(dx,(0), axis=0) # Dropping the first period which does not contain any information anymore.\n",
    "    # Every dirst-difference means that we will lose one time period.\n",
    "    #dx = numpy.delete(dx,(0), axis=1)\n",
    "d2x = np.zeros(dx.shape) #Constructing a zero variable for the first-difference command which is following (this is necessary)\n",
    "d2x[1:] = dx[1:] - dx[:-1] # Second-Difference\n",
    "d2x = np.delete(d2x,(0), axis=0)\n",
    "    #For verification command out the next lines:\n",
    "    #dx_df = pd.DataFrame(dx)\n",
    "    #print dx_df.head(n=5) # The first entry is now zero for all zero because we can't take the first-difference from the starting value. Comparing dx_df and x_df one can clearly see that the first-differnce worked well.\n",
    "    # Now we have the reshape the data again in long form for the regression\n",
    "dx_reshape = dx # result is stored for the within-transformation    \n",
    "dx = np.reshape(dx.T, N-n) # replacing dx with long form (to be used in a regression)\n",
    "# We have to substract n from N, because we delete one time period\n",
    "d2x = np.reshape(d2x.T, N-2*n)\n",
    "# Same here, just that now we deleted two time periods.\n",
    "    #dx[:11] #to check if transition worked\n",
    "\n",
    "# Within-transformation for x\n",
    "dx_mean = dx_reshape.mean(axis=0) # obtaining the mean for each individual after the first-difference is done\n",
    "dx_mean_i = np.repeat(dx_mean,T-1) # for the calculation, construct a mean-matrix with same shape as the x-variable\n",
    "dx_mean_i_reshape = np.reshape(dx_mean_i.T, N-n) # Reshaping the data into long-form\n",
    "dwx = dX - dx_mean_i_reshape # Substracting the mean for the within-fransformation:\n",
    "# dwX is now constructed for the Random-Trend 2, mix    \n",
    "    \n",
    "################ Some transition for the y-variable\n",
    "y_reshape = np.reshape(np.ravel(y_it), (200, 5)).T # reshape x-variable to wide form for the transformation\n",
    "dy = np.zeros(y_reshape.shape) #Constructing a zero variable for the first-difference command which is following (this is necessary)\n",
    "dy[1:] = y_reshape[1:] - y_reshape[:-1] # First-Difference\n",
    "dy = np.delete(dy,(0), axis=0) # Dropping the first period which does not contain any information nomore.\n",
    "\n",
    "d2y = np.zeros(dy.shape) #Constructing a zero variable for the first-difference command which is following (this is necessary)\n",
    "d2y[1:] = dy[1:] - dy[:-1] # Second-Difference\n",
    "d2y = np.delete(d2y,(0), axis=0)\n",
    "dy_reshape = dy # result is stored for the within-transformation\n",
    "dy = np.reshape(dy.T, N-n)\n",
    "d2y = np.reshape(d2y.T, N-2*n)\n",
    "\n",
    "dy_mean = dy_reshape.mean(axis=0) \n",
    "dy_mean_i = np.repeat(dy_mean,T-1) \n",
    "dy_mean_i_reshape = np.reshape(dy_mean_i.T, N-n) \n",
    "dwy = dY - dy_mean_i_reshape \n",
    "# dwY is now constructed for the Random-Trend 2, mix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Already noting this transformation part, it does seem very reasonable to rely on Stata if one wants to analyze panel data. Surely, this code is not as efficient as possible since we just started coding, and we are certain that the code can be improved, but Stata seems so far still more convenient to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            FD   RT-pure    RT-mix\n",
      "Beta  2.999467  2.005803  2.277722\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######### Regresison\n",
    "# First-Difference\n",
    "x = np.mat(dx).T\n",
    "y = np.mat(dy).T\n",
    "estimate_fd = (x.T * x).I * x.T * y\n",
    "\n",
    "x2 = np.mat(d2x).T\n",
    "y2 = np.mat(d2y).T\n",
    "estimate_pure = (x2.T * x2).I * x2.T * y2\n",
    "\n",
    "xw = np.mat(dwx).T\n",
    "yw = np.mat(dwy).T\n",
    "estimate_mix = (xw.T * xw).I * xw.T * yw\n",
    "\n",
    "results = np.array((estimate_fd, estimate_pure, estimate_mix))\n",
    "results = np.mat(results)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = [\"FD\", \"RT-pure\", \"RT-mix\",]\n",
    "results_df.index = [\"Beta\"]\n",
    "print results_df\n",
    "\n",
    "########################## Possible OLS-regression packages we decided against. We kept in the code just in case we want to find it somewhen later on.\n",
    "#model = sm.OLS(dy, dx)\n",
    "#results = model.fit()\n",
    "#print(results.summary())\n",
    "#print('Parameters: ', results.params)\n",
    "\n",
    "#model = sm.OLS(d2y, d2x)\n",
    "#results = model.fit()\n",
    "#print(results.summary())\n",
    "#print('Parameters: ', results.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The results show for the first two entries a similar picture to what we had in in the Stata results as well. The FD is upwards biased due to the linear specific trend. The _pure_ estimation solves for the engogeneity problem and gives a coefficient very close to the true value (difference is only at the 3. digit which should play a dig role). For the _mix_, where we first calculate the first-difference and then subtract the mean specific to each individual performs not as good as before. The coeffcient is equal to 2.277722, so roughly biased by 0.28. Looking into the Stata help file of the [xtreg](http://www.stata.com/manuals13/xtxtreg.pdf) command shows us that in Stata does not just uses the within-transformation, but also adds the overall average:\n",
    "\n",
    "$$y_{it} - \\bar{y}_{i} + \\bar{\\bar{y}} = \\beta \\cdot (x_{it} - \\bar{x}_{i} + \\bar{\\bar{x}}) + \\alpha_i - \\alpha_i + \\bar{\\alpha} + e_{it} - \\bar{e}_{i} +\\bar{\\bar{e}} \\quad $$ \n",
    "\n",
    "with $\\bar{y}_{i} = \\sum_{t=1}^T y_{it}$, $\\bar{\\bar{y}} = \\sum_{i}\\sum_{t} x_{it} /(nT_i)$\n",
    "\n",
    "This might influence the results. Another possibility would be that the seed was just not _in favor_ for the _mix_ method.\n",
    "\n",
    "As noted before, we will run now the regression with 1000 replications to see how the estimations behaves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 -5        -1       0.5         2\n",
      "FD_beta   -4.000154 -0.000163  1.499854  2.999825\n",
      "FD_se      0.000308  0.000323  0.000304  0.000294\n",
      "RT-1_beta -4.999154 -0.998878  0.499683  1.998566\n",
      "RT-1_se    0.050218  0.052277  0.050720  0.051731\n",
      "RT-2_beta -4.999304 -0.999507  0.499016  1.999729\n",
      "RT-2_se    0.042489  0.043701  0.043235  0.043062\n"
     ]
    }
   ],
   "source": [
    "r = 1000 # number of replications of the simulation\n",
    "\n",
    "#### empty matrices generated to store the results at the end of the loop\n",
    "# for first-difference\n",
    "estimate_fd = np.mat(np.empty((1, r))) \n",
    "estimate_mat_fd = np.mat(np.empty((4, r))) \n",
    "beta_fd_hat = np.zeros(4)\n",
    "se_fd_hat = np.zeros(4)\n",
    "\n",
    "# for random-trend (1) (pure)\n",
    "estimate_rt_p = np.mat(np.empty((1, r)))\n",
    "estimate_mat_p = np.mat(np.empty((4, r))) \n",
    "beta_rt_p_hat = np.zeros(4)\n",
    "se_rt_p_hat = np.zeros(4)\n",
    "\n",
    "# for random-trend (2) (mix)\n",
    "estimate_rt_m = np.mat(np.empty((1, r)))\n",
    "estimate_mat_m = np.mat(np.empty((4, r))) \n",
    "beta_rt_m_hat = np.zeros(4)\n",
    "se_rt_m_hat = np.zeros(4)\n",
    "####\n",
    "\n",
    "l = 0 # Used to move within a vector to store results at the end\n",
    "n = 200 # defines the number of individuals\n",
    "T = 5 # defines the number of time periods\n",
    "N = n * T # maximum number of observations, necessary for the calculations\n",
    "intercept = 3 # Even though the intercept cancels out, we leave it in the model because it was in the basic model\n",
    "for beta in (-5, -1, 0.5, 2): # Here we actually run the code vor reveral beta's, -5, -1, 0.5 and 2\n",
    "    # in stata it would be `forvalue beta in (-5, -1, 0.5, 2)`\n",
    "    for j in range(0, r): # Loop for the different amounts of replications of the simulation, done for each beta     \n",
    "        # DGP\n",
    "        random.seed(r) # Seed is defined in the loop to have a different seed per replication, but still knowing which seed's are used\n",
    "        alpha_i = np.random.normal(0, 1, n)\n",
    "        alpha_i = np.repeat(alpha_i,T) \n",
    "        i = np.arange(n)\n",
    "        i = np.repeat(i,T)\n",
    "        t = np.arange(T) + 1 \n",
    "        t = np.tile(t,n)\n",
    "        trend = t*i \n",
    "        mean = [0, 0]\n",
    "        cov = [[1, 0], [0, 1]]\n",
    "        nu_it, e_it = np.random.multivariate_normal(mean, cov, N).T \n",
    "        x_it = (nu_it + alpha_i + trend).T \n",
    "        y_it = (intercept + alpha_i + beta*x_it + trend + e_it).T # <=> `g y_it = 3 + alpha_i + 2*x_it + trend + e_it `\n",
    "        \n",
    "        ############################### Transformation for the first-difference and randon-trend (1) and (2)\n",
    "        \n",
    "        # First-difference and random-trend (1)\n",
    "        x_reshape = np.reshape(np.ravel(x_it), (n, T)).T \n",
    "        dx = np.zeros(x_reshape.shape)\n",
    "        dx[1:] = x_reshape[1:] - x_reshape[:-1] \n",
    "        dx = np.delete(dx,(0), axis=0) \n",
    "        dx_reshape = dx # result is stored for the within-transformation\n",
    "        d2x = np.zeros(dx.shape) \n",
    "        d2x[1:] = dx[1:] - dx[:-1] \n",
    "        d2x = np.delete(d2x,(0), axis=0)\n",
    "        \n",
    "        dX = np.reshape(dx.T, N-n) \n",
    "        d2X = np.reshape(d2x.T, N-2*n)\n",
    "\n",
    "        y_reshape = np.reshape(np.ravel(y_it), (n, T)).T \n",
    "        dy = np.zeros(y_reshape.shape)\n",
    "        dy[1:] = y_reshape[1:] - y_reshape[:-1] \n",
    "        dy = np.delete(dy,(0), axis=0)\n",
    "        dy_reshape = dy # result is stored for the within-transformation\n",
    "        d2y = np.zeros(dy.shape)\n",
    "        d2y[1:] = dy[1:] - dy[:-1] \n",
    "        d2y = np.delete(d2y,(0), axis=0)\n",
    "        dY = np.reshape(dy.T, N-n)\n",
    "        d2Y = np.reshape(d2y.T, N-2*n)\n",
    "        \n",
    "        # Within-transformation\n",
    "        dx_mean = dx_reshape.mean(axis=0) \n",
    "        dx_mean_i = np.repeat(dx_mean,T-1) \n",
    "        dx_mean_i_reshape = np.reshape(dx_mean_i.T, N-n) \n",
    "        dwX = dX - dx_mean_i_reshape\n",
    "        # dwX is now constructed for the Random-Trend 2, mix\n",
    "        # Same for the y-variable\n",
    "        dy_mean = dy_reshape.mean(axis=0) \n",
    "        dy_mean_i = np.repeat(dy_mean,T-1) \n",
    "        dy_mean_i_reshape = np.reshape(dy_mean_i.T, N-n) \n",
    "        dwY = dY - dy_mean_i_reshape \n",
    "        # dwY is now constructed for the Random-Trend 2, mix\n",
    "        \n",
    "        ################# First-Difference Regression\n",
    "        X = np.mat(dX).T\n",
    "        Y = np.mat(dY).T\n",
    "        M = (X.T * X).I * X.T\n",
    "        estimate_fd[:, j] = M * Y\n",
    "        estimate_mat_fd[l] = estimate_fd\n",
    "        \n",
    "        ################# Random-Trend (1), pure Regression\n",
    "        X2 = np.mat(d2X).T\n",
    "        Y2 = np.mat(d2Y).T\n",
    "        M2 = (X2.T * X2).I * X2.T\n",
    "        eestimate_rt_p[:, j] = M2 * Y2\n",
    "        estimate_mat_p[l] = estimate_rt_p\n",
    "        \n",
    "        ################# Random-Trend (2), mix Regression\n",
    "        Xw = np.mat(dwX).T\n",
    "        Yw = np.mat(dwY).T\n",
    "        Mw = (Xw.T * Xw).I * Xw.T\n",
    "        estimate_rt_m[:, j] = Mw * Yw \n",
    "        estimate_mat_m[l] = estimate_rt_m\n",
    "        \n",
    "        \n",
    "        \n",
    "    mu_fd = estimate_fd.mean()\n",
    "    var_fd = estimate_fd.var()\n",
    "    sigma_fd = np.sqrt(var_fd)\n",
    "    beta_fd_hat[l] = mu_fd\n",
    "    se_fd_hat[l] = sigma_fd\n",
    "    \n",
    "    mu_rt_p = estimate_rt_p.mean()\n",
    "    var_rt_p = estimate_rt_p.var()\n",
    "    sigma_rt_p = np.sqrt(var_rt_p)\n",
    "    beta_rt_p_hat[l] = mu_rt_p\n",
    "    se_rt_p_hat[l] = sigma_rt_p\n",
    "    \n",
    "    mu_rt_m = estimate_rt_m.mean()\n",
    "    var_rt_m = estimate_rt_m.var()\n",
    "    sigma_rt_m = np.sqrt(var_rt_m)\n",
    "    beta_rt_m_hat[l] = mu_rt_m\n",
    "    se_rt_m_hat[l] = sigma_rt_m\n",
    "    \n",
    "    l = l + 1\n",
    "    \n",
    "\n",
    "#estimate_fd = np.squeeze(np.asarray(estimate_fd))\n",
    "\n",
    "\n",
    "#estimate_rt_p = np.squeeze(np.asarray(estimate_rt_p))\n",
    "\n",
    "\n",
    "#estimate_rt_m = np.squeeze(np.asarray(estimate_rt_m))\n",
    "\n",
    "\n",
    "results = np.matrix((beta_fd_hat, se_fd_hat, beta_rt_p_hat, se_rt_p_hat, beta_rt_m_hat, se_rt_m_hat))\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.index = [\"FD_beta\", \"FD_se\", \"RT-1_beta\", \"RT-1_se\", \"RT-2_beta\", \"RT-2_se\"]\n",
    "results_df.columns = [\"-5\", \"-1\", \"0.5\", \"2\"]\n",
    "print results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output table `results` one can see the results of the estimated coefficient $\\beta$ for the first-difference and random-trend _pure_ and _mix_. Further, we included the the standard deviation of the distribution the estimated coefficients have from the various replication (so, this is not a mean from each standard error, but just the one standard error from the coefficients. This goes more into the direction of bootstrapping(?)). The numbers of each columns show the true value of the coefficient.\n",
    "\n",
    "First we see that the now the _mix_ (=2) RT is not much different from the _pure_ estimation.\n",
    "\n",
    "For a better overview of the results, the next cells will do some comparisons. First we calculate the differences of the estimated $\\hat{\\beta}$ (given by beta_fd_hat for the first-difference) and the true $\\beta$ (given by beta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = np.array((-5, -1, 0.5, 2))\n",
    "diff_fd = beta_fd_hat - beta\n",
    "diff_fd_r = np.round(np.absolute(diff_fd),0)\n",
    "\n",
    "diff_rt_p = beta_rt_p_hat - beta\n",
    "diff_rt_p_r = np.round(np.absolute(diff_rt_p),0)\n",
    "\n",
    "diff_rt_m = beta_rt_m_hat - beta\n",
    "diff_rt_m_r = np.round(np.absolute(diff_rt_m),0)\n",
    "\n",
    "diff_se_rt = se_rt_p_hat - se_rt_m_hat        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see whether the differences from the true coefficient is large, we compare the estimate to the value $0.01$ and let python tell us whether the difference is bigger or smaller than the chosen value.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test if the difference between estimator of the first-difference and the true value is significant unequal to zero\n",
      "Reject\n",
      "Reject\n",
      "Reject\n",
      "Reject\n",
      "\n",
      "Test if the difference between estimator of the random-trend (1) and the true value is significant unequal to zero\n",
      "Don't reject\n",
      "Don't reject\n",
      "Don't reject\n",
      "Don't reject\n",
      "\n",
      "Test if the difference between estimator of the random-trend (2) and the true value is significant unequal to zero\n",
      "Don't reject\n",
      "Don't reject\n",
      "Don't reject\n",
      "Don't reject\n"
     ]
    }
   ],
   "source": [
    "print \"\"\n",
    "print \"Test if the difference between estimator of the first-difference and the true value is significant unequal to zero\" \n",
    "for l in range(0,4):\n",
    "    if diff_fd[l] / se_fd_hat[l] > 1.96:\n",
    "        print \"Reject\"\n",
    "    else:\n",
    "        print \"Don't reject\"  \n",
    "print \"\"\n",
    "print \"Test if the difference between estimator of the random-trend (1) and the true value is significant unequal to zero\"         \n",
    "for l in range(0,4):\n",
    "    if diff_rt_m[l] / se_rt_m_hat[l] > 1.96:\n",
    "        print \"Reject\"\n",
    "    else:\n",
    "        print \"Don't reject\" \n",
    "\n",
    "print \"\"\n",
    "print \"Test if the difference between estimator of the random-trend (2) and the true value is significant unequal to zero\"                 \n",
    "for l in range(0,4):\n",
    "    if diff_rt_m[l] / se_rt_m_hat[l] > 1.96:\n",
    "        print \"Reject\"\n",
    "    else:\n",
    "        print \"Don't reject\"         \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test we used the 95\\%-quantile. We conclude that both, the _pure_ and the _mix_ random-trend give consistent estimates. The first-difference (as expected) gives us inconsistent results. Thereby we conferm the results we obtained from stata above. It seems that the code conducted by us works. Works in the sense that the transformations are done correctly.\n",
    "\n",
    "To see however whether _pure_ or _mix_ is better, we will see which difference of the coefficient is smaller and which standard-errors are smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which method produces a smaller differences with the true value?\n",
      "Mix\n",
      "Mix\n",
      "Mix\n",
      "Pure\n",
      "\n",
      "Which method produces smaller standard errors?\n",
      "Mix\n",
      "Mix\n",
      "Mix\n",
      "Mix\n",
      "\n",
      "Difference of the standard errors:\n",
      "[ 0.008  0.009  0.007  0.009]\n"
     ]
    }
   ],
   "source": [
    "print \"Which method produces a smaller differences with the true value?\"\n",
    "for l in range(0,4):\n",
    "    if diff_rt_p[l] < diff_rt_m[l]:\n",
    "        print \"Pure\"\n",
    "    else:\n",
    "        print \"Mix\" \n",
    "\n",
    "print \"\"        \n",
    "print \"Which method produces smaller standard errors?\"        \n",
    "for l in range(0,4):\n",
    "    if se_rt_p_hat[l] < se_rt_m_hat[l]:\n",
    "        print \"Pure\"\n",
    "    else:\n",
    "        print \"Mix\" \n",
    "        \n",
    "print \"\"\n",
    "print \"Difference of the standard errors:\"\n",
    "print diff_se_rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our simulation, it does not seem that for the mean of the coefficient it makes a different whether ones uses the first or the second random-trend approach. Here, two times the _mix_-approach has a smaller difference and two times the _pure_-approach has a smaller difference. The differences overall are very small, so small that in empirical papers it should not matter.\n",
    "\n",
    "For the standard error however, always the _mix_-approach is preferred. This means that the for the 1000 replications, the _mix_-approach is more accurant than the _pure_-approach. The difference of the standard errors is at the second decimal digit which, from our point of view, does not seem to be small enough to neglet it.\n",
    "\n",
    "We therefore conclude that it might be reasonable to suggest to use the second random-trend approach, in which one calculates first the first-difference to cancel out the constant and tranform the linear trend into a constant and then takes use of the within-transformation to cancel out the remaining constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/matplotlib/axes/_axes.py:5907: UserWarning: 2D hist input should be nsamples x nvariables;\n",
      " this looks transposed (shape is 2 x 1000)\n",
      "  '(shape is %d x %d)' % inp.shape[::-1])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEZCAYAAACq1zMoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxf8PqIzRkUnEU4MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a2c9d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEZCAYAAABxbJkKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf2KG0xMKW334AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a772190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#axis = np.round(estimate,4)\n",
    "#h = plt.hist(np.round(estimate,2))    \n",
    "n, bins, patches = plt.hist(np.round(estimate_fd,6), 15, normed=1, facecolor='g', alpha=0.85)\n",
    "#plt.axis([2.9981, 3.0009, 0, 25])\n",
    "plt.xlabel('Coefficient of variable x'); plt.ylabel('Density'); plt.title('Histogram Estimation Results')\n",
    "plt.plot(bins, 1/(sigma_fd * np.sqrt(2 * np.pi)) *\n",
    "               np.exp( - (bins - mu_fd)**2 / (2 * sigma_fd**2) ),\n",
    "         linewidth=2, color='r')\n",
    "#plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "plt.show()\n",
    "\n",
    "#axis = np.round(estimate,4)\n",
    "#h = plt.hist(np.round(estimate,2))    \n",
    "n, bins, patches = plt.hist(np.round(estimate_rt,6), 15, normed=1, facecolor='g', alpha=0.85)\n",
    "#plt.axis([2.9981, 3.0009, 0, 25])\n",
    "plt.xlabel('Coefficient of random trend model'); plt.ylabel('Density'); plt.title('Histogram Estimation Results')\n",
    "#plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "plt.show()\n",
    "\n",
    "#diff = beta - estimate_fd\n",
    "#h = plt.hist(diff)    \n",
    "#n, bins, patches = plt.hist(estimate, 12, normed=1, facecolor='g', alpha=0.85)\n",
    "#plt.axis([2.9981, 3.0009, 0, 25])\n",
    "#plt.xlabel('Coefficient of variable x'); plt.ylabel('Density'); plt.title('Histogram Estimation Results')\n",
    "#plt.text(20, -1.0005, r'$\\mu=100,\\ \\sigma=15$')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e3511d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mu = 0\n",
    "sigma = 1\n",
    "s = np.random.normal(0, 1, 200)\n",
    "count, bins, ignored = plt.hist(s, 30, normed=True)\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
    "         linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
