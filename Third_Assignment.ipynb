{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Third Assignment\n",
    "\n",
    "Carlos\n",
    "Daniel\n",
    "Jakob Schwerter (110583)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things that have to be done:\n",
    "    - Getting the standard errors in all simulations of the estimation (not the estimator!)\n",
    "    - Nice plots for all the results to picture the results. \n",
    "    - Make plots flexible to paramter changes (for us n, t and beta)\n",
    "    - If possible, make the code faster / easier\n",
    "    - Count the time the stata and comparable python code needs\n",
    "    - Comment on which program to prefer\n",
    "    - Check typos\n",
    "    - Loading a dataset to check the simulation results for a empirical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipystata # https://github.com/TiesdeKok/ipystata - We kind of helped so that ipystata works on mac as well :)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pip install ipystata --upgrade --force-reinstall\n",
    "# Check if a new version is online before running the code.\n",
    "# Might be useful and does not take much time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Motivation of the Assignment\n",
    "\n",
    "The idea of the assignment is to construct a simulation for the random-trend model (Wooldrige 2010, section 11.7.1). \n",
    "The random-trend (or growth) is an extension of the fixed-effects model and thereby a panel-data method.\n",
    "The main interest is to find an unbiased estimator for variables which are biased trough a random-trend. Besides the unobserved constant ($\\alpha_i$) the variable of interest ($x_it$) is correlated with a linear trend ($g_i \\cdot t$). For this random-trend model, the literatur just states that in the first step we have to calculate the first-difference to transform the linear trend into a constant. Thereafter we it is open to the researcher to continue with the within-tranformation or the first-difference (even though both are a fixed-effect method, we will be consistent with the literatur to call the within-transformation the fixed-effect method). We will investigate if one of the approaches is superior to the other in means of the (bias of the) coefficient [???, standard errors and model selection criteria ($R^2$, AIC and BIC)???].\n",
    "\n",
    "We thereby have two possibles to calculate the random-trend model: \n",
    "- Taking two time the first-difference.\n",
    "- First the first-difference, then the fixed-effect.\n",
    "\n",
    "The true DGP is as follows:\n",
    "\n",
    "$y_{it} = \\beta \\cdot x_{it} + \\alpha_i + g_i \\cdot t + e_{it} $\n",
    "\n",
    "$y_{it}$ is our dependent variable we wish to explain. $x_{it}$ is our variable of interest which we want to estimate, thereby $\\beta$ is our parameter of interest. $\\alpha_i$ is the constant (un)observable, $g_i \\cdot t$ the linear trend (un)observable. $e_{it}$ the idiosyncratic error term which can vary randomly over the individual and periods.\n",
    "\n",
    "Taking the first-difference gives\n",
    "\n",
    "$y_{it} - y_{it-1} = \\beta \\cdot ( x_{it} - x_{it-1}) + \\alpha_i - \\alpha_{i} +   g_i \\cdot t - g_{i} \\cdot (t-1) + (e_{it} - e_{it-1})$\n",
    "\n",
    "$ \\iff \\Delta y_{it} = \\beta \\cdot \\Delta x_{it} + g_i + \\Delta  e_{it}$\n",
    "\n",
    "One can see that the linear trend is reduced to a constant term which can be canceld out by a second fixed-effects transformation.\n",
    "\n",
    "Fixed-Effects (i.e. subtracting each varibale by the mean respective to the individual):\n",
    "\n",
    "$\\Delta y'_{it} = \\beta \\cdot \\Delta x'_{it} + \\Delta e'_{it}$, whereby ' denotes that the variables are demeanded.\n",
    "\n",
    "First-Difference:\n",
    "\n",
    "$\\Delta^2 y_{it} = \\beta \\cdot \\Delta^2 x_{it} + \\Delta^2 e_{it}$\n",
    "\n",
    "\n",
    "Taking first the fixed-effects would give the following:\n",
    "\n",
    "$y_{it} - \\bar{y}_{i} = \\beta \\cdot ( x_{it} - \\bar{x}_{i}) + \\alpha_i - \\alpha_{i} +   g_i \\cdot (t - \\bar{t}) + (e_{it} - \\bar{e}_{i})$\n",
    "\n",
    "Using a second transformation we will not be able to cancel out the time trend effect (we will confirm this in one simulation at some point).\n",
    "\n",
    "Given that our group is new to python but experienced in stata, we will actually first do the simulation using stata. We run stata codes using the package `ipystata`. Thereby we have a known language which we can refer to our benchmark. In a second step we will use only open software packages to replicate the results. Thereby we will comment which python code is comparable to which stata code. Despide having the stata-results as a benchmark, we start a nice _translater_ from stata to python for those who are new to python but know stata. We will also command on the speed of both languages as well as advantages and disadvanteges of the coding part.\n",
    "\n",
    "The assignment will continue as follows: (i) First we will explain step by step our data generating process (DGP). (ii) Then we ran a simulation without a random trend to see if everything works fine. (ii) Next we run the same simulation using a constant linear trend and (iii) a individual specific trend in the data generating process. To see how robust the random-trend estimates are, we will further run two simulations using two non-linear trends.\n",
    "After that we will reproduce the results of the individual trend using open-source packages only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stata\n",
    "\n",
    "## Explanation of the Data Generating Process (DGP)\n",
    "We will first explain the main part of the simulation process. Minor changes or additions however will be denoted later in the code. In _Stata_ commenting within the code is done using \" // \"\n",
    "\n",
    "The first two cells are to call the prgroam Stata itself and to see if it actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipystata.config import config_stata\n",
    "config_stata('/Applications/Stata/StataSE.app/Contents/MacOS/StataSE') \n",
    "\n",
    "# Windows  --> 'C:\\Program Files (x86)\\Stata14\\StataSE-64.exe'\n",
    "# Mac OS X --> '/Applications/Stata/StataSE.app/Contents/MacOS/stataSE'\n",
    "# Linux    --> '/home/user/stata14/stata-se'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, I am printed by Stata\n",
      "We welcome you to our assignment and we hope you enjoy it\n"
     ]
    }
   ],
   "source": [
    "%%stata \n",
    "display \"Hello, I am printed by Stata\" \n",
    "display \"We welcome you to our assignment and we hope you enjoy it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/jakobschwerter/Dropbox/github/python/Stata/Assignment\n"
     ]
    }
   ],
   "source": [
    "%%stata \n",
    "cd \"/Users/jakobschwerter/Dropbox/github/python/Stata/Assignment\" \n",
    "set seed 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First we have to _call_ stata using `%%stata`. Then we tell stata where it should (i) look for files and (ii) saves files with the command `cd`.\n",
    "\n",
    "> We further set the seed to 100, so that results are easier to be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(obs 200)\n"
     ]
    }
   ],
   "source": [
    "%%stata -o simulation\n",
    "drawnorm alpha_i, n(200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">First we create a new dataset called _simulation_ using `-o`. This is necessary for `ipystata` on mac.\n",
    "\n",
    ">Then we draw 200 iid random numbers of a standard normal distribution with mean zero and variance 1. The give that variable the name $alpha_i$ ($\\alpha_i$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(800 observations created)\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation -o simulation\n",
    "expand 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> `-d` calls the dataset _simulation_ , otherwise the commands afterwards would not find any variables. The command `-o simulation` saves the changes.\n",
    "\n",
    "> We expand the generated data by 5, i.e. obtainig every alpha 4 additional times\n",
    "The vector thereby expands from $(200 \\; \\text{x} \\; 1)$ to $(200 \\cdot 5 \\; \\text{x} \\; 1)$. Thereby we have a variable which is constant over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation -o simulation\n",
    "drawnorm nu_it e_it, n(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Draw two new variables (or vectors) with now $1000 (5 \\cdot 200)$ entries. Two names mean two different variables from a multivariate normal distribution.\n",
    "Stata first generates one variable with 1000 draws and then the second.\n",
    " 1000 draws so that it fit with the number of alpha's which where expanded by 5. Both variables are independent from each other, so they are not correlated. The data is iid as well.\n",
    " \n",
    "> To check if we did everything correct, we have a look into summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Variable |       Obs        Mean    Std. Dev.       Min        Max\n",
      "-------------+--------------------------------------------------------\n",
      "       index |      1000       499.5    288.8194          0        999\n",
      "     alpha_i |      1000   -.0376166    1.014596  -2.885089    2.10282\n",
      "       nu_it |      1000    .0101147    1.027895  -2.885089   3.081883\n",
      "        e_it |      1000   -.0310172    .9936022  -2.741925   3.359478\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation\n",
    "sum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We see the three generated variables $\\alpha_i, \\; nu_{it}, \\; e_{it}$, having a mean very close to zero and a standard deviation close to 1. Further, using stata in python, and index variable is generated automatically. The index variable would be the individual variable if we would run a pooled panel regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation -o simulation\n",
    "g x_it = nu_it + alpha_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Generats our variable of interest $x_{it}$ using two existing variables. Since $\\alpha_i$ is a compenent, $x_{it}$ will be correlated over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation -o simulation\n",
    "g y_it=3+alpha_i+2*x_it+e_it // DGP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Generats our dependent variable $y_{it}$. We use\n",
    " an intercept, $\\alpha_i$, $x_{it}$ and $e_{it}$ for that. 2 is the assigned parameter to $x_{it}$.\n",
    " \n",
    ">The underlying model is a panel data model, consisting of 200 individuals and 5 time periods per person. It is completely balanced. The variables $y_{it}$ and $x_{it}$ are correlated with a constant term $\\alpha_i$. This produces autocorrelation within an individual.\n",
    "\n",
    "\n",
    "\n",
    ">Checking if the code worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             |    index  alpha_i    nu_it     e_it     x_it     y_it\n",
      "-------------+------------------------------------------------------\n",
      "       index |   1.0000 \n",
      "             |\n",
      "             |\n",
      "     alpha_i |   0.0079   1.0000 \n",
      "             |   0.8022\n",
      "             |\n",
      "       nu_it |   0.0283   0.2159   1.0000 \n",
      "             |   0.3711   0.0000\n",
      "             |\n",
      "        e_it |  -0.0082  -0.0082  -0.0185   1.0000 \n",
      "             |   0.7967   0.7949   0.5600\n",
      "             |\n",
      "        x_it |   0.0233   0.7764   0.7830  -0.0172   1.0000 \n",
      "             |   0.4612   0.0000   0.0000   0.5880\n",
      "             |\n",
      "        y_it |   0.0180   0.8425   0.6525   0.2253   0.9579   1.0000 \n",
      "             |   0.5701   0.0000   0.0000   0.0000   0.0000\n",
      "             |\n",
      "An output file must be specified\n"
     ]
    }
   ],
   "source": [
    "%%stata -d simulation\n",
    "pwcorr,sig\n",
    "corrtex alpha_i e_it x_it y_it, sig "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The command _pwcorr_ shows a correlation table of the variables in the dataset (which were generated previously). The option _sig_ prints significance level for each correlation. Results are presented above. $x_it$ and $\\alpha_i$ are statistical significant correlated (77.64\\%). Not including or solving for $\\alpha_i$ when regressing $x_{it}$ on $y_{it}$ will induce a bias. The correlation of $x_{it}$ on $y_{it}$ and $\\alpha_i$ is natural very high, since it depends on both variables.\n",
    " The correlation of $e_{it}$ with $x_{it}$ and $\\alpha_i$ is very low and statistical insignificant, which is necessary and by construction. An OLS model like $y_{it} = \\beta x_{it}$  will be upward biased since the correlation of $\\alpha_i$ and $x_{it}$ is positive. $\\beta$ will estimate the effect of $\\alpha$ and $x_{it}$. If the correlation would be negative, it would have been downward biased.\n",
    "\n",
    "> To check if the DGP is correct, we do one simulation without imposing a trend into the DGP.\n",
    "\n",
    "# Simulation without Random Trend in the true DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">         b(%7,5f) se(%6.3f) scalars(\"N N\" \"r2_a r2_a)\" \"F F-Stat\" \"p p-value\" \"r2 r2\" \"aic aic\" \"bic\") sfmt(%4,0f %6,5f %5,2f %5,4f %6,5f %9,2f %9,2f) star(+ 0.1 * 0.05 ** 0.01 *** 0.001) ///\n",
      ">         label mtitles( \"OLS\" \"FE\" \"FD\" \"FD-FE\" \"2-FD\" \"FD-FD\" ) title(\"Without a random trend component\") nolabel replace \n",
      "\n",
      "Without a random trend component\n",
      "----------------------------------------------------------------------------------------------\n",
      "                       (1)          (2)          (3)          (4)          (5)          (6)   \n",
      "                       OLS           FE           FD        FD-FE         2-FD        FD-FD   \n",
      "----------------------------------------------------------------------------------------------\n",
      "x_it               2,47702***   2,02807***   2,03600***   2,03628***   2,02678***   2,02678***\n",
      "                   (0.033)      (0.036)      (0.044)      (0.046)      (0.056)      (0.056)   \n",
      "Constant           2,96331***   3,01220***   0,02163      0,02164***   0,05001      0,05001   \n",
      "                   (0.050)      (0.004)      (0.025)      (0.002)      (0.049)      (0.049)   \n",
      "----------------------------------------------------------------------------------------------\n",
      "Observations          1000         1000          800          800          600          600   \n",
      "r2_a)              0,88590      0,79106      0,78723      0,78583      0,79025      0,79025   \n",
      "F-Stat             5621,67      3127,46      2120,07      1941,12      1297,89      1297,89   \n",
      "p-value             0,0000            .       0,0000            .       0,0000       0,0000   \n",
      "r2                 0,88602      0,79127      0,78749      0,78610      0,79060      0,79060   \n",
      "aic                3256,78      2685,09      2904,96      2854,71      2830,15      2830,15   \n",
      "bic                3266,60      2690,00      2914,33      2859,40      2838,94      2838,94   \n",
      "----------------------------------------------------------------------------------------------\n",
      "Standard errors in parentheses\n",
      "+ p<0.1, * p<0.05, ** p<0.01, *** p<0.001\n",
      "Time needed to run:\n",
      "   1:      1.20 /        1 =       1.2020\n"
     ]
    }
   ],
   "source": [
    "%%stata -o sim\n",
    "timer on 1\n",
    "qui set seed 100\n",
    "qui drawnorm alpha_i, n(200)\n",
    "qui gen i = _n // included additional to generate an index for the individual level\n",
    "qui expand 5\n",
    "qui bys i: g t = _n // included additional to generate an index for the time level\n",
    "qui drawnorm nu_it e_it, n(1000)\n",
    "qui g x_it = nu_it + alpha_i\n",
    "qui g y_it = 3 + alpha_i + 2*x_it + e_it // DGP\n",
    "\n",
    "qui xtset i t //give stata panel-information\n",
    "\n",
    "qui gen dx_it = d.x_it // generates the first-difference prior to the regression command\n",
    "qui gen dy_it = d.y_it // generates the first-difference prior to the regression command\n",
    "\n",
    "\n",
    "qui eststo: reg y_it x_it, cluster(i) // OLS-regression (biased)\n",
    "qui eststo: xtreg  y_it x_it, fe cluster(i)    // FE-regression\n",
    "qui eststo: reg d.y_it d.x_it, cluster(i) // FD-regression\n",
    "qui eststo: xtreg d.y_it d.x_it,fe cluster(i) // Random-Trend\n",
    "qui eststo: reg d2.y_it d2.x_it, cluster(i) // Random-Trend  (2 FD)\n",
    "qui eststo: reg d.dy_it d.dx_it, cluster(i) // Random-Trend  (FD-FD)    \n",
    "\n",
    "esttab, long compress nogaps rename(D.x_it x_it D2.x_it x_it D.dx_it x_it Constant C) ///\n",
    "\tb(%7,5f) se(%6.3f) scalars(\"N N\" \"r2_a r2_a)\" \"F F-Stat\" \"p p-value\" \"r2 r2\" \"aic aic\" \"bic\") sfmt(%4,0f %6,5f %5,2f %5,4f %6,5f %9,2f %9,2f) star(+ 0.1 * 0.05 ** 0.01 *** 0.001) ///\n",
    "\tlabel mtitles( \"OLS\" \"FE\" \"FD\" \"FD-FE\" \"2-FD\" \"FD-FD\" ) title(\"Without a random trend component\") nolabel replace \n",
    "timer off 1\n",
    "display \"Time needed to run:\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that the results of the normal OLS are biased, i.e. the coefficient for $x_{it}$ is not 2 but around 2.4. For the within-transformation and the first-difference however the coefficient if very close to 2.\n",
    "Also the random-trend is unbiased, but it seems that the second transformation worsens the results. It is farthermost of 2 and also the standard error is the highest. Also the F-Stat, adjusted $R^2$, $R^2$ and AIC decreased (Just for the AIC and BIC, the FE is the worst). So we can conclude that even if we do not have a random-trend component, using two transformations does not really harm the estimation.\n",
    "\n",
    "> Comparing the different Random Trend calculations, we see (i) the the mixture seem to be more efficient, but the pure is closer to the true value (2) and (ii) that (5) and (6) are excatly the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Study without Trend\n",
    "\n",
    "In a simulation however, results from just one run can be misleading, because the results can be driven by chance, i.e. if the seed in use just draws numbers which workes better for one method than the other. Therefore, we will run a programm now to do a monte carlo study. We will take 1000 draws to see if above resusts are robust.\n",
    "\n",
    "New in the code is (i) a timer which measure the time the code runs. The necessary commands are `timer on 1`, starting the timer, called '1', `timer off 1`, stopping the started timer and `timer list 1`, displaying the time needed.\n",
    "(ii) For the simulation commands `simulation`, we first have to embed the code in a program. The first line `capture program drop mcprog_ols` drops if somehow the program is already defined. This command is useful if you run the code an additional time. `program mcprog_ols` starts the program and ends with `end`.\n",
    "\n",
    "Then the simulation just simulate the coefficients saved in stata in `_b` (given the time, we focus on the coefficients and leave out the standard errors). We take 1000 replications and we call the earlier defined programm `mcprog_pool` (Monte Carlo Program - Pooled Regression). \n",
    "\n",
    "At the end we rename the coefficient for a better understanding and drop everything else from the dataframe (`keep beta_pool`).\n",
    "\n",
    "Using `-np` only suppresses any output. We will compare the combined output later on.\n",
    "\n",
    "_pool_ stands for the pooled regress, _fe_ for the within-transformation, _fd_ for the first-difference, _mix_ for the random-trend, using first-difference and fixed-effects and _pure_ the random trend with two first-differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:     14.27 /        1 =      14.2700\n"
     ]
    }
   ],
   "source": [
    "%%stata -o pool \n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pool\n",
    "program mcprog_pool\n",
    "         clear\n",
    "         drawnorm alpha_i, n(200)\n",
    "         gen i = _n // included additional to generate an index for the individual level\n",
    "         expand 5\n",
    "         bys i: g t = _n // included additional to generate an index for the time level\n",
    "         drawnorm nu_it e_it, n(1000)\n",
    "         g x_it=nu_it+alpha_i\n",
    "         g y_it=3+alpha_i+2*x_it+e_it\n",
    "         reg y_it x_it, // cluster(i) Take out the clustered standard errors because we focus on the coefficients\n",
    "         drop nu_it alpha_i e_it y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(1000): mcprog_pool\n",
    "        rename _b_x_it beta_pool\n",
    "        keep beta_pool\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%stata -o fe\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "         clear\n",
    "         drawnorm alpha_i, n(200)\n",
    "         gen i = _n // included additional to generate an index for the individual level\n",
    "         expand 5\n",
    "         bys i: g t = _n // included additional to generate an index for the time level\n",
    "         drawnorm nu_it e_it, n(1000)\n",
    "         g x_it=nu_it+alpha_i\n",
    "         g y_it=3+alpha_i+2*x_it+e_it\n",
    "         xtset i t\n",
    "         xtreg y_it x_it, fe // cluster(i)\n",
    "         drop nu_it alpha_i e_it y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(1000): mcprog_fe\n",
    "        rename _b_x_it beta_fe\n",
    "        keep beta_fe\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%stata -o fd\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fd\n",
    "        program mcprog_fd\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it=nu_it+alpha_i\n",
    "        g y_it=3+alpha_i+2*x_it+e_it\n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it, // cluster(i)\n",
    "        drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(1000): mcprog_fd\n",
    "        rename _b_dx_it beta_fd\n",
    "        keep beta_fd\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%stata -o mix\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_mix\n",
    "        program mcprog_mix\n",
    "         clear\n",
    "         drawnorm alpha_i, n(200)\n",
    "         gen i = _n // included additional to generate an index for the individual level\n",
    "         expand 5\n",
    "         bys i: g t = _n // included additional to generate an index for the time level\n",
    "         drawnorm nu_it e_it, n(1000)\n",
    "         g x_it=nu_it+alpha_i\n",
    "        g y_it=3+alpha_i+2*x_it+e_it\n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "         xtreg dy_it dx_it, fe // cluster(i)\n",
    "         drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(1000): mcprog_mix\n",
    "        rename _b_dx_it beta_mix\n",
    "        keep beta_mix\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%stata -o pure\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pure\n",
    "        program mcprog_pure\n",
    "         clear\n",
    "         drawnorm alpha_i, n(200)\n",
    "         gen i = _n // included additional to generate an index for the individual level\n",
    "         expand 5\n",
    "         bys i: g t = _n // included additional to generate an index for the time level\n",
    "         drawnorm nu_it e_it, n(1000)\n",
    "         g x_it=nu_it+alpha_i\n",
    "         g y_it=3+alpha_i+2*x_it+e_it\n",
    "        xtset i t\n",
    "        g d2x_it = d2.x_it\n",
    "        g d2y_it = d2.y_it\n",
    "         reg d2y_it d2x_it, // cluster(i)\n",
    "         drop nu_it alpha_i e_it y_it x_it d2y_it d2x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(1000): mcprog_pure\n",
    "        rename _b_d2x_it beta_pure\n",
    "        keep beta_pure\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.49910474],\n",
       "       [ 2.001513  ],\n",
       "       [ 2.00073147],\n",
       "       [ 2.00057745],\n",
       "       [ 1.99920857]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_mean = pool.mean()\n",
    "fe_mean = fe.mean()\n",
    "fd_mean = fd.mean()\n",
    "mix_mean = mix.mean()\n",
    "pure_mean = pure.mean()\n",
    "\n",
    "mean = [pool_mean, fe_mean, fd_mean, mix_mean, pure_mean]\n",
    "mean\n",
    "meanv = np.array([pool_mean, fe_mean, fd_mean, mix_mean, pure_mean])\n",
    "meanv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that the pooled regression is upward biased form the true value (2). Closest to the true are (??? -should calulate the difference to the true here???)\n",
    "Further, we could include histograms, but I could not do it easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Monte Carlo Study with the same trend for all individuals\n",
    "\n",
    "Here we investigate what happens to our estimator is we include the same trend for all individual. If we would just run normal regression as presented first, we could just use the same datasets, replacing just some variables. But since we have to define programs, we need to copy all the programs and change for the trend. We will leave out the pooled regression because we already know, that this one is biased just due to the constant $\\alpha_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:      6.90 /        1 =       6.9040\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fe_st\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + t // NOW: x_it is correlated with a linear term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + t + e_it // DGP includes know the linear trend\n",
    "        xtset i t\n",
    "        xtreg y_it x_it, fe // cluster(i)\n",
    "        drop nu_it alpha_i e_it y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_x_it beta_fe_st\n",
    "        keep beta_fe_st // st = same trend\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:      1.98 /        1 =       1.9790\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fd_st\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + t // NOW: x_it is correlated with a linear term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + t + e_it // DGP includes know the linear trend\n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it, // cluster(i)\n",
    "        drop nu_it alpha_i e_it y_it x_it dy_it dx_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_dx_it beta_fd_st\n",
    "        keep beta_fd_st\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      6.30 /        1 =       6.3000\n"
     ]
    }
   ],
   "source": [
    "%%stata -o mix_st\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_mix\n",
    "        program mcprog_mix\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + t // NOW: x_it is correlated with a linear term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + t + e_it // DGP includes know the linear trend\n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        xtreg dy_it dx_it, fe // cluster(i)\n",
    "        drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_mix\n",
    "        rename _b_dx_it beta_mix_st\n",
    "        keep beta_mix_st\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      1.97 /        1 =       1.9690\n"
     ]
    }
   ],
   "source": [
    "%%stata -o pure_st\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pure\n",
    "        program mcprog_pure\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + t // NOW: x_it is correlated with a linear term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + t + e_it // DGP includes know the linear trend\n",
    "        xtset i t\n",
    "        g d2x_it = d2.x_it\n",
    "        g d2y_it = d2.y_it\n",
    "        reg d2y_it d2x_it, // cluster(i)\n",
    "        drop nu_it alpha_i e_it y_it x_it d2y_it d2x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_pure\n",
    "        rename _b_d2x_it beta_pure_st\n",
    "        keep beta_pure_st\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.71409535],\n",
       "       [ 2.00397825],\n",
       "       [ 2.00418329],\n",
       "       [ 2.00259447]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_st_mean = fe_st.mean()\n",
    "fd_st_mean = fd_st.mean()\n",
    "mix_st_mean = mix_st.mean()\n",
    "pure_st_mean = pure_st.mean()\n",
    "\n",
    "\n",
    "meanv = np.array([fe_st_mean, fd_st_mean, mix_st_mean, pure_st_mean])\n",
    "meanv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">The simulation shows that if the trend is the same for all individuals in the data, the within-transformation cannot capture the linear trend. Thereby the coefficent is biased. Suprisingly the first-difference is enough to solve for the problem introduced by a correlation of the variable of interest and the trend. We will not take a deeper look here, but it might be interest to investigate how many individuals are allowed to differ so that the first-difference is not unbiased anymore (or should we?)\n",
    "It seems that the First-Difference is more robust here. It is however questionable in practice if the observed individuals really follow the same trend.\n",
    "\n",
    ">Further, the first-difference performs better than the mixe, but worse than the pure.\n",
    "\n",
    "# Monte Carlo study with individual specific trends (Random Trend)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:      5.39 /        1 =       5.3900\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fe_it\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = t*i // individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend // NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        xtreg y_it x_it, fe // cluster(i)\n",
    "        drop nu_it alpha_i e_it y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_x_it beta_fe_it\n",
    "        keep beta_fe_it // it = individual trend\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:      2.31 /        1 =       2.3130\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fd_it\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        gen trend = t*i // individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend // NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it, // cluster(i)\n",
    "        drop nu_it alpha_i e_it y_it x_it dy_it dx_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_dx_it beta_fd_it\n",
    "        keep beta_fd_it\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      6.58 /        1 =       6.5770\n"
     ]
    }
   ],
   "source": [
    "%%stata -o mix_it\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_mix\n",
    "        program mcprog_mix\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        gen trend = t*i // individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend // NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        xtreg dy_it dx_it, fe // cluster(i)\n",
    "        drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_mix\n",
    "        rename _b_dx_it beta_mix_it\n",
    "        keep beta_mix_it\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      2.15 /        1 =       2.1460\n"
     ]
    }
   ],
   "source": [
    "%%stata -o pure_it\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pure\n",
    "        program mcprog_pure\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        gen trend = t*i // individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend // NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g d2x_it = d2.x_it\n",
    "        g d2y_it = d2.y_it\n",
    "        reg d2y_it d2x_it, // cluster(i)\n",
    "        drop nu_it alpha_i e_it y_it x_it d2y_it d2x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_pure\n",
    "        rename _b_d2x_it beta_pure_it\n",
    "        keep beta_pure_it\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.99995708],\n",
       "       [ 2.9994812 ],\n",
       "       [ 2.00418329],\n",
       "       [ 2.00259447]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_it_mean = fe_it.mean()\n",
    "fd_it_mean = fd_it.mean()\n",
    "mix_it_mean = mix_it.mean()\n",
    "pure_it_mean = pure_it.mean()\n",
    "\n",
    "\n",
    "meanv = np.array([fe_it_mean, fd_it_mean, mix_it_mean, pure_it_mean])\n",
    "meanv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The results confirm (unsurprisingly) the bad results of the within-tranformation. Now, also the first-difference is not capable to control for the trend within $x_{it}$. Only unbiased estimations are _mix_ and _pure_. The results are the same as before (remember: same seed). After checking that twe really changed the code, it can be concluded that for the random-trend model it does not matter whether the trend is constant or individual specific.\n",
    "\n",
    "> In this easy example it further seems that the double difference (_pure_) is more robust, i.e. closer to the true value. Thereby the example is in favor to use the _pure_ rather than the _mix_ random-trend method. Also the time point, the _pure_ is much faster.\n",
    "\n",
    "# Monte Carlo study with nonlinear individual specific trends (Random Trend)\n",
    "\n",
    "We will check for two cases, an exponential individual specific trend and a log individual specific trend. The motivation is if one of the random-trend methods performs better even if the baseline assumption, that the trend is linear, does not hold.\n",
    "\n",
    "### Monte Carlo study with exponential individual specific trends (Random Trend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:      2.33 /        1 =       2.3270\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fd_exp\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        gen trend = exp(t*i) // individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend // NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it, // cluster(i)\n",
    "        drop nu_it alpha_i e_it y_it x_it dy_it dx_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_dx_it beta_fd_exp\n",
    "        keep beta_fd_exp\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      6.16 /        1 =       6.1560\n"
     ]
    }
   ],
   "source": [
    "%%stata -o mix_exp\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_mix\n",
    "        program mcprog_mix\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        gen trend = exp(t*i) // individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend // NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        xtreg dy_it dx_it, fe // cluster(i)\n",
    "        drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_mix\n",
    "        rename _b_dx_it beta_mix_exp\n",
    "        keep beta_mix_exp\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      2.43 /        1 =       2.4290\n"
     ]
    }
   ],
   "source": [
    "%%stata -o pure_exp\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pure\n",
    "        program mcprog_pure\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        gen trend = exp(t*i) // individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend // NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g d2x_it = d2.x_it\n",
    "        g d2y_it = d2.y_it\n",
    "        reg d2y_it d2x_it, // cluster(i)\n",
    "        drop nu_it alpha_i e_it y_it x_it d2y_it d2x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_pure\n",
    "        rename _b_d2x_it beta_pure_exp\n",
    "        keep beta_pure_exp\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.        ],\n",
       "       [ 3.00000024],\n",
       "       [ 3.        ]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_exp_mean = fd_exp.mean()\n",
    "mix_exp_mean = mix_exp.mean()\n",
    "pure_exp_mean = pure_exp.mean()\n",
    "\n",
    "meanv = np.array([fd_exp_mean, mix_exp_mean, pure_exp_mean])\n",
    "meanv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we have the case that the trend increases individual specific exponential over time, i.e. convex\n",
    "Both are biased, and the difference between the two is at the 7. digit, so obsolete. Suprisingly to us is, that there is no difference between the normal first-difference. This means that if the trend is not linear, the random trend method does not improve the estimation. I would have expected that the rantom-trend model comes a bite closer to the true, but in this example it does not.\n",
    "\n",
    "> ### Monte Carlo study with exponential individual specific trends (Random Trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:      3.98 /        1 =       3.9800\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fd_log\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = log(t*i) // logarithm !!! \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it\n",
    "        drop nu_it alpha_i e_it y_it x_it dy_it dx_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_dx_it beta_fd_log\n",
    "        keep beta_fd_log\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      7.34 /        1 =       7.3440\n"
     ]
    }
   ],
   "source": [
    "%%stata -o mix_log\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_mix\n",
    "        program mcprog_mix\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = log(t*i) // logarithm!\n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        xtreg dy_it dx_it, fe // cluster(i)\n",
    "        drop nu_it alpha_i e_it dy_it dx_it y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_mix\n",
    "        rename _b_dx_it beta_mix_log\n",
    "        keep beta_mix_log\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". timer off 1\n",
      "The time this codes run is\n",
      "   1:      2.27 /        1 =       2.2740\n"
     ]
    }
   ],
   "source": [
    "%%stata -o pure_log\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_pure\n",
    "        program mcprog_pure\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n // included additional to generate an index for the individual level\n",
    "        expand 5\n",
    "        bys i: g t = _n // included additional to generate an index for the time level\n",
    "        gen trend = log(t*i) // individual specific trand   \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend // NOW: x_it is correlated with a linear specific term\n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        g d2x_it = d2.x_it\n",
    "        g d2y_it = d2.y_it\n",
    "        reg d2y_it d2x_it, // cluster(i)\n",
    "        drop nu_it alpha_i e_it y_it x_it d2y_it d2x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_pure\n",
    "        rename _b_d2x_it beta_pure_log\n",
    "        keep beta_pure_log\n",
    "        \n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.02005911],\n",
       "       [ 2.02132511],\n",
       "       [ 2.00435495]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_log_mean = fd_log.mean()\n",
    "mix_log_mean = mix_log.mean()\n",
    "pure_log_mean = pure_log.mean()\n",
    "\n",
    "meanv = np.array([fd_log_mean, mix_log_mean, pure_log_mean])\n",
    "meanv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we have the case that the trend is upward sloping, but the marginal increase decreases, i.e. we have cancove trends.\n",
    "All three methods are suprisingly good in capturing the _unobserved_ trend component. As for the constant trend, the _mixe_ performs worst, _fd_ second and _pure_ performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The not-working within-transformation first-step: \n",
    "At the very top we stated that it does not make any sense to first transform the data using the within-transformation and then using the first-difference. For the canceling of the trend, it was crucial to take first the first-difference. We will investigate this in the following using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:     10.57 /        1 =      10.5670\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fe_fe\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = t*i  \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        xtdata i t x_it y_it, fe clear // preforms the within-transformation, so we can be sure that it is really the first thing in the regression\n",
    "        xtreg y_it x_it, fe //  Random-Trend (FE-FD)\n",
    "        drop y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_x_it beta_fe_fe\n",
    "        keep beta_fe_fe // it = individual trend\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time this codes run is\n",
      "   1:      3.69 /        1 =       3.6920\n"
     ]
    }
   ],
   "source": [
    "%%stata -o fe_fd\n",
    "timer on 1\n",
    "set seed 100\n",
    "capture program drop mcprog_fe\n",
    "        program mcprog_fe\n",
    "        clear\n",
    "        drawnorm alpha_i, n(200)\n",
    "        gen i = _n \n",
    "        expand 5\n",
    "        bys i: g t = _n \n",
    "        gen trend = t*i  \n",
    "        drawnorm nu_it e_it, n(1000)\n",
    "        g x_it = nu_it + alpha_i + trend \n",
    "        g y_it = 3 + alpha_i + 2*x_it + trend + e_it \n",
    "        xtset i t\n",
    "        xtdata i t x_it y_it, fe clear // preforms the within-transformation, so we can be sure that it is really the first thing in the regression\n",
    "        xtset i t // to sort the data\n",
    "        g dx_it = d.x_it\n",
    "        g dy_it = d.y_it\n",
    "        reg dy_it dx_it,  // Random-Trend (FE-FD)\n",
    "        drop y_it x_it\n",
    "end\n",
    "        simulate _b, nodots nolegend  reps(100): mcprog_fe\n",
    "        rename _b_dx_it beta_fe_fd\n",
    "        keep beta_fe_fd // it = individual trend\n",
    "timer off 1\n",
    "display \"The time this codes run is\"\n",
    "timer list 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.99995708],\n",
       "       [ 2.9994812 ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_fe_mean = fe_fe.mean()\n",
    "fe_fd_mean = fe_fd.mean()\n",
    "\n",
    "meanv = np.array([fe_fe_mean, fe_fd_mean])\n",
    "meanv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Results are similar to the normal within-tranformation. Doing the second transformation is not helpful at all, i.e. the results confirm the theoretical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The same using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import scipy as sp\n",
    "from pandas import *\n",
    "import pandas\n",
    "#import rpy2.robjects as ro\n",
    "import random\n",
    "\n",
    "#import numpy.random as R\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "#import pylab as P\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Individual  Time   alpha_i  trend         y_it        x_it      e_it\n",
      "0             1     1 -0.315619      1     5.966723    0.938004  0.406333\n",
      "1             1     2 -0.315619      2    10.755261    2.813945  0.442990\n",
      "2             1     3 -0.315619      3     7.405965    1.378846 -1.036109\n",
      "3             1     4 -0.315619      4    14.477653    4.473510 -1.153749\n",
      "4             1     5 -0.315619      5    15.647115    3.662680  0.637374\n",
      "5             2     1 -0.451261      2     7.723998    1.533979  0.107301\n",
      "6             2     2 -0.451261      4    16.808943    4.416874  1.426456\n",
      "7             2     3 -0.451261      6    21.083685    6.690712 -0.846477\n",
      "8             2     4 -0.451261      8    25.771535    7.709322 -0.195847\n",
      "9             2     5 -0.451261     10    31.494125   10.567407 -2.189428\n",
      "10            3     1  0.983558      3    10.620450    2.318532 -1.000171\n",
      "11            3     2  0.983558      6    24.865002    7.089636  0.702173\n",
      "12            3     3  0.983558      9    36.127754   11.508935  0.126326\n",
      "13            3     4  0.983558     12    44.172457   14.281514 -0.374129\n",
      "14            3     5  0.983558     15    54.709226   17.569823  0.586023\n",
      "15            4     1  0.059631      4    19.227044    6.301467 -0.435521\n",
      "16            4     2  0.059631      8    25.131449    7.101045 -0.130272\n",
      "17            4     3  0.059631     12    45.087969   13.616062  2.796214\n",
      "18            4     4  0.059631     16    51.514576   16.264929 -0.074912\n",
      "19            4     5  0.059631     20    68.043212   21.567387  1.848807\n",
      "20            5     1  0.643015      5    17.154219    4.076013  0.359179\n",
      "21            5     2  0.643015     10    35.083558   10.079757  1.281030\n",
      "22            5     3  0.643015     15    50.035960   15.188337  1.016272\n",
      "23            5     4  0.643015     20    63.470534   19.948240 -0.068961\n",
      "24            5     5  0.643015     25    77.335433   24.156609  0.379201\n",
      "25            6     1 -1.639359      6    19.119571    5.714306  0.330319\n",
      "26            6     2 -1.639359     12    32.700032   10.307497 -1.275602\n",
      "27            6     3 -1.639359     18    54.495770   17.581111 -0.027094\n",
      "28            6     4 -1.639359     24    71.761877   23.154238  0.092760\n",
      "29            6     5 -1.639359     30    90.861300   29.878937 -0.257216\n",
      "..          ...   ...       ...    ...          ...         ...       ...\n",
      "970         195     1  0.115249    195   584.163220  192.139436  1.769099\n",
      "971         195     2  0.115249    390  1173.517951  391.134804 -1.866907\n",
      "972         195     3  0.115249    585  1757.725828  584.635223  0.340134\n",
      "973         195     4  0.115249    780  2342.229836  779.473266  0.168054\n",
      "974         195     5  0.115249    975  2926.787504  974.868918 -1.065581\n",
      "975         196     1  0.294247    196   589.291379  195.915089 -1.833045\n",
      "976         196     2  0.294247    392  1179.744909  392.012463  0.425737\n",
      "977         196     3  0.294247    588  1768.137548  588.416142  0.011018\n",
      "978         196     4  0.294247    784  2355.748868  783.939151  0.576320\n",
      "979         196     5  0.294247    980  2942.810427  978.987381  1.541419\n",
      "980         197     1 -0.895012    197   591.752802  195.848526  0.950761\n",
      "981         197     2 -0.895012    394  1182.381528  393.077480  0.121580\n",
      "982         197     3 -0.895012    591  1775.911273  592.024751 -1.243217\n",
      "983         197     4 -0.895012    788  2365.003159  788.059729 -1.221286\n",
      "984         197     5 -0.895012    985  2957.245296  984.788939  0.562430\n",
      "985         198     1  2.330758    198   600.591067  198.944610 -0.628911\n",
      "986         198     2  2.330758    396  1197.102374  398.275333 -0.779052\n",
      "987         198     3  2.330758    594  1792.987567  596.748420  0.159969\n",
      "988         198     4  2.330758    792  2386.393883  794.440355  0.182414\n",
      "989         198     5  2.330758    990  2976.554494  990.438317  0.347103\n",
      "990         199     1  1.102723    199   600.516614  198.839384 -0.264877\n",
      "991         199     2  1.102723    398  1197.846238  397.650863  0.441790\n",
      "992         199     3  1.102723    597  1797.077438  598.492802 -1.010889\n",
      "993         199     4  1.102723    796  2393.279760  797.463670 -1.750304\n",
      "994         199     5  1.102723    995  2992.216389  996.660751 -0.207837\n",
      "995         200     1 -0.190925    200   603.067243  199.913669  0.430830\n",
      "996         200     2 -0.190925    400  1199.741780  399.443464 -1.954223\n",
      "997         200     3 -0.190925    600  1801.503471  599.251292  0.191813\n",
      "998         200     4 -0.190925    800  2401.976458  799.657661 -0.147939\n",
      "999         200     5 -0.190925   1000  3001.187465  999.122964  0.132461\n",
      "\n",
      "[1000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "random.seed(100)\n",
    "n = 200\n",
    "T = 5\n",
    "N = n * T\n",
    "alpha_i = np.random.normal(0, 1, 200) # Equivalent to `drawnorm alpha_i, n(200)`\n",
    "alpha_i = np.repeat(alpha_i,5) # Equivalent to `expand 5`  \n",
    "\n",
    "# Equivalent for the 'expand 5' command, so we will have 5 time periods\n",
    "i = ro.r('seq(1,200,1)')# Using sequence generator from R\n",
    "\n",
    "i = np.array(i) # Equivalent to `gen i = _n`\n",
    "i = np.repeat(i,5)\n",
    "\n",
    "t = np.arange(5) + 1 # Equivalent to bys i: g t = _n \n",
    "t = np.tile(t,200)\n",
    "#t.shape\n",
    "trend = t*i # <=> `gen trend = t*i`\n",
    "\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "nu_it, e_it = np.random.multivariate_normal(mean, cov, 1000).T # <=> `drawnorm nu_it e_it, n(1000)`\n",
    "\n",
    "x_it = (nu_it + alpha_i + trend).T # <=> `g x_it = nu_it + alpha_i + trend`\n",
    "# Since we will transform the data at least ones, we don't have to include an intercept in the first place.\n",
    "\n",
    "y_it = (3 + alpha_i + 2*x_it + trend + e_it).T # <=> `g y_it = 3 + alpha_i + 2*x_it + trend + e_it `\n",
    "\n",
    "\n",
    "\n",
    "m = np.matrix((i, t, alpha_i, trend, y_it, x_it, e_it))\n",
    "mt = m.transpose()\n",
    "\n",
    "df = pd.DataFrame(mt)\n",
    "df.columns = [\"Individual\", \"Time\", \"alpha_i\", \"trend\", \"y_it\", \"x_it\", \"e_it\"]\n",
    "#print(df)\n",
    "\n",
    "#np.set_printoptions(precision=3, suppress=True)\n",
    "#print(mt)\n",
    "#y_it.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data generating process is here now down completely using open source packages. The arising problem now however is, that our internet research did not provide us bith commands which can analyse panel data as easily as stata can do. I.e. there is no command which is directly comparable to \n",
    "`xtset i t` and `xtreg y_it x_it, fe`. We therefor have to construct the transformations ourselfs. We will start with taking the first-difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEZCAYAAACNebLAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e07c0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Parameters: ', array([ 2.99971809]))\n",
      "('Parameters: ', array([ 2.00802103]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_reshape = np.reshape(np.ravel(x_it), (200, 5)).T # reshape x-variable to wide form for the transformation\n",
    "    # To verify the reshape command, comment out the next to lines. We need 200 columsn with each 5 rows for 200 individuals haven 5 time periods\n",
    "    #x_df = pd.DataFrame(x_reshape)\n",
    "    #print x_df.head(n=5) # One can see that we have now 200 columns with each 5 time periods\n",
    "dx = np.zeros(x_reshape.shape) #Constructing a zero variable for the first-difference command which is following (this is necessary)\n",
    "dx[1:] = x_reshape[1:] - x_reshape[:-1] # First-Difference\n",
    "dx = np.delete(dx,(0), axis=0) # Dropping the first period which does not contain any information anymore.\n",
    "    # Every dirst-difference means that we will lose one time period.\n",
    "    #dx = numpy.delete(dx,(0), axis=1)\n",
    "d2x = np.zeros(dx.shape) #Constructing a zero variable for the first-difference command which is following (this is necessary)\n",
    "d2x[1:] = dx[1:] - dx[:-1] # Second-Difference\n",
    "d2x = np.delete(d2x,(0), axis=0)\n",
    "    # For verification command out the next lines:\n",
    "    #dx_df = pd.DataFrame(dx)\n",
    "    #print dx_df.head(n=5) # The first entry is now zero for all zero because we can't take the first-difference from the starting value. Comparing dx_df and x_df one can clearly see that the first-differnce worked well.\n",
    "    # Now we have the reshape the data again in long form for the regression\n",
    "dx = np.reshape(dx.T, N-n) # replacing dx with long form (to be used in a regression)\n",
    "# We have to substract n from N, because we delete one time period\n",
    "d2x = np.reshape(d2x.T, N-2*n)\n",
    "# Same here, just that now we deleted two time periods.\n",
    "    #dx[:11] #to check if transition worked\n",
    "\n",
    "################ Some transition for the y-variable\n",
    "y_reshape = np.reshape(np.ravel(y_it), (200, 5)).T # reshape x-variable to wide form for the transformation\n",
    "dy = np.zeros(y_reshape.shape) #Constructing a zero variable for the first-difference command which is following (this is necessary)\n",
    "dy[1:] = y_reshape[1:] - y_reshape[:-1] # First-Difference\n",
    "dy = np.delete(dy,(0), axis=0) # Dropping the first period which does not contain any information nomore.\n",
    "\n",
    "d2y = np.zeros(dy.shape) #Constructing a zero variable for the first-difference command which is following (this is necessary)\n",
    "d2y[1:] = dy[1:] - dy[:-1] # Second-Difference\n",
    "d2y = np.delete(d2y,(0), axis=0)\n",
    "\n",
    "dy = np.reshape(dy.T, N-n)\n",
    "d2y = np.reshape(d2y.T, N-2*n)\n",
    "\n",
    "x = np.mat(dx).T\n",
    "y = np.mat(dy).T\n",
    "estimate = (x.T * x).I * x.T * y\n",
    "\n",
    "\n",
    "\n",
    "model = sm.OLS(dy, dx)\n",
    "results = model.fit()\n",
    "#print(results.summary())\n",
    "print('Parameters: ', results.params)\n",
    "\n",
    "\n",
    "\n",
    "model = sm.OLS(d2y, d2x)\n",
    "results = model.fit()\n",
    "#print(results.summary())\n",
    "print('Parameters: ', results.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.]]\n",
      "(800, 1)\n",
      "(800, 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can clearly see that the first regression, where we just use one difference, is biased as it was in the stata code. For the second estimation, we get and unbiased estimator. It seems that the transformation using python (or numpy) worked. This however was just for one simulation. As we did before, we should now repeat the simulation several times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEZCAYAAABB4IgrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY2Zmhfv/fwrGRG4gk1QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10aa09e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEZCAYAAAC99aPhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a822090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = 100 # number of replications\n",
    "n = 200\n",
    "T = 5\n",
    "N = n * T\n",
    "intercept = 3\n",
    "beta = 0.5\n",
    "estimate = np.mat(np.empty((1, r)))\n",
    "for j in range(0, r):\n",
    "    # DGP\n",
    "    random.seed(r)\n",
    "    alpha_i = np.random.normal(0, 1, n)\n",
    "    alpha_i = np.repeat(alpha_i,T) \n",
    "    i = np.arange(n)\n",
    "    i = np.repeat(i,T)\n",
    "    t = np.arange(T) + 1 \n",
    "    t = np.tile(t,n)\n",
    "    trend = t*i \n",
    "    mean = [0, 0]\n",
    "    cov = [[1, 0], [0, 1]]\n",
    "    nu_it, e_it = np.random.multivariate_normal(mean, cov, N).T \n",
    "    x_it = (nu_it + alpha_i + trend).T \n",
    "    y_it = (intercept + alpha_i + beta*x_it + trend + e_it).T # <=> `g y_it = 3 + alpha_i + 2*x_it + trend + e_it `\n",
    "    ########## Transformation\n",
    "    x_reshape = np.reshape(np.ravel(x_it), (n, T)).T \n",
    "    dx = np.zeros(x_reshape.shape)\n",
    "    dx[1:] = x_reshape[1:] - x_reshape[:-1] \n",
    "    dx = np.delete(dx,(0), axis=0) \n",
    "    d2x = np.zeros(dx.shape) \n",
    "    d2x[1:] = dx[1:] - dx[:-1] \n",
    "    d2x = np.delete(d2x,(0), axis=0)\n",
    "    dx = np.reshape(dx.T, N-n) \n",
    "    d2x = np.reshape(d2x.T, N-2*n)\n",
    "\n",
    "    y_reshape = np.reshape(np.ravel(y_it), (n, T)).T \n",
    "    dy = np.zeros(y_reshape.shape)\n",
    "    dy[1:] = y_reshape[1:] - y_reshape[:-1] \n",
    "    dy = np.delete(dy,(0), axis=0) \n",
    "    d2y = np.zeros(dy.shape)\n",
    "    d2y[1:] = dy[1:] - dy[:-1] \n",
    "    d2y = np.delete(d2y,(0), axis=0)\n",
    "    dy = np.reshape(dy.T, N-n)\n",
    "    d2y = np.reshape(d2y.T, N-2*n)\n",
    "    x = np.mat(dx).T\n",
    "    y = np.mat(dy).T\n",
    "    M = (x.T * x).I * x.T\n",
    "    estimate[:, j] = M * y\n",
    "        \n",
    "mu = estimate.mean()\n",
    "var = estimate.var()\n",
    "sigma = np.sqrt(var)\n",
    "estimate = np.squeeze(np.asarray(estimate))\n",
    "\n",
    "#axis = np.round(estimate,4)\n",
    "#h = plt.hist(np.round(estimate,2))    \n",
    "n, bins, patches = plt.hist(np.round(estimate,6), 15, normed=1, facecolor='g', alpha=0.85)\n",
    "#plt.axis([2.9981, 3.0009, 0, 25])\n",
    "plt.xlabel('Coefficient of variable x'); plt.ylabel('Density'); plt.title('Histogram Estimation Results')\n",
    "#plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "plt.show()\n",
    "\n",
    "diff = beta - estimate\n",
    "h = plt.hist(diff)    \n",
    "#n, bins, patches = plt.hist(estimate, 12, normed=1, facecolor='g', alpha=0.85)\n",
    "#plt.axis([2.9981, 3.0009, 0, 25])\n",
    "plt.xlabel('Coefficient of variable x'); plt.ylabel('Density'); plt.title('Histogram Estimation Results')\n",
    "#plt.text(20, -1.0005, r'$\\mu=100,\\ \\sigma=15$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = ro.r('seq(1,200,1)')\n",
    "#print i\n",
    "m = np.arange(200)\n",
    "#print m\n",
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(estimate)\n",
    "diff = 0.5 - estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "estimate2 = np.mat(np.zeros((1,10)))\n",
    "estimate2[:, 1] = 1\n",
    "estimate2[:, 2] = 2\n",
    "print estimate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e3511d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mu = 0\n",
    "sigma = 1\n",
    "s = np.random.normal(0, 1, 200)\n",
    "count, bins, ignored = plt.hist(s, 30, normed=True)\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
    "         linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mu, sigma = 100, 15\n",
    "x = mu + sigma * np.random.randn(1000)\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(estimate, 50, normed=1, facecolor='g', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.xlabel('Smarts')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Histogram of IQ')\n",
    "#plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "#plt.axis([40, 160, 0, 0.03])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
